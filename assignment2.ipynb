{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Import some packages that we need to use\n",
    "from utils import *\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='FrozenLake8x8-v1', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            # [TODO] run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            if render:\n",
    "                _render_helper(env)\n",
    "            obs, reward, done, info = env.step(policy(obs))\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break   \n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# [TODO] Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='FrozenLake8x8-v1', model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.env.P[state][act]\n",
    "\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('FrozenLake')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v1 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class SARSATrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v1'\n",
    "                 ):\n",
    "        super(SARSATrainer, self).__init__(env_name, model_based=False)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # epsilon-greedy exploration policy parameter\n",
    "        self.eps = eps\n",
    "\n",
    "        # maximum steps in single episode\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        # [TODO] uncomment the next line, pay attention to the shape\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: We have self.eps probability to choose a unifomly random\n",
    "        #  action in range [0, 1, .., self.action_dim - 1], \n",
    "        #  otherwise choose action that maximize the Q value\n",
    "        if np.random.random()<self.eps:\n",
    "            return np.random.choice(4)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] Q table may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        \n",
    "        # No, we should do nothing.\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            next_act = self.policy(next_obs)\n",
    "\n",
    "            # [TODO] compute the TD error, based on the next observation and\n",
    "            #  action.\n",
    "            td_error = reward+self.gamma*self.table[next_obs][next_act]-self.table[obs][act]\n",
    "            \n",
    "            # [TODO] compute the new Q value\n",
    "            # hint: use TD error, self.learning_rate and old Q value\n",
    "            new_value = self.table[obs][act]+self.learning_rate*td_error\n",
    "            \n",
    "            self.table[obs][act] = new_value\n",
    "\n",
    "            # [TODO] Implement (1) break if done. (2) update obs for next \n",
    "            #  self.policy(obs) call\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                obs=next_obs             \n",
    "\n",
    "# [TODO] run the next cell to check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# set eps = 0 to disable exploration.\n",
    "test_trainer = SARSATrainer(eps=0.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "# set the Q value of (obs 0, act 3) to 100, so that it should be taken by \n",
    "# policy.\n",
    "test_obs = 0\n",
    "test_act = test_trainer.action_dim - 1\n",
    "test_trainer.table[test_obs][test_act] = 100\n",
    "\n",
    "# assertion\n",
    "assert test_trainer.policy(test_obs) == test_act, \\\n",
    "    \"Your action is wrong! Should be {} but get {}.\".format(\n",
    "        test_act, test_trainer.policy(test_obs))\n",
    "\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "# set eps = 0 to disable exploitation.\n",
    "test_trainer = SARSATrainer(eps=1.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "act_set = set()\n",
    "for i in range(100):\n",
    "    act_set.add(test_trainer.policy(0))\n",
    "\n",
    "# assertion\n",
    "assert len(act_set) > 1, (\"You sure your uniformaly action selection mechanism\"\n",
    "                          \" is working? You only take action {} when \"\n",
    "                          \"observation is 0, though we run trainer.policy() \"\n",
    "                          \"for 100 times.\".format(act_set))\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "print(\"Policy Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_sarsa_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def sarsa(train_config=None):\n",
    "    config = default_sarsa_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = SARSATrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.634.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.641.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.672.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.665.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.647.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.65.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.647.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.655.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.685.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.642.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.668.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.637.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.629.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.675.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.664.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.621.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.662.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.638.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.675.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.124|0.122|0.059|0.001|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.162|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.235|0.245|0.326|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.497|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.164|0.000|0.039|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.236|0.000|0.431|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.484|0.734|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.497|0.703|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.080|0.020|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.339|0.498|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.728|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.125|0.062|0.001|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.125|0.000|0.004|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.169|0.000|0.198|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.357|0.485|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class QLearningTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v1'\n",
    "                 ):\n",
    "        super(QLearningTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: Just copy your codes in SARSATrainer.policy()\n",
    "        if np.random.random()<self.eps:\n",
    "            return np.random.choice(4)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # [TODO] Q table may be need to be reset to zeros.\n",
    "        # if you think it should, than do it. If not, then move on.\n",
    "        pass\n",
    "        # No, we should do nothing.\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        for t in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "            # [TODO] compute the TD error, based on the next observation\n",
    "            # hint: we do not need next_act anymore.\n",
    "            td_error = reward+self.gamma*np.max(self.table[next_obs])-self.table[obs][act]\n",
    "            \n",
    "            # [TODO] compute the new Q value\n",
    "            # hint: use TD error, self.learning_rate and old Q value\n",
    "            new_value = self.table[obs][act]+self.learning_rate*td_error\n",
    "            \n",
    "            self.table[obs][act] = new_value\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_q_learning_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.01,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def q_learning(train_config=None):\n",
    "    config = default_q_learning_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = QLearningTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.667.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.661.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.627.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.662.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.641.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.652.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.667.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.66.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.647.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.643.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.667.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.689.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.663.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.627.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.65.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.657.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.262|0.026|0.026|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.328|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.410|0.410|0.512|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.328|0.000|0.416|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.410|0.000|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.640|0.800|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.532|0.800|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.210|0.164|0.001|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.512|0.640|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.800|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.134|0.021|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.262|0.000|0.117|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.328|0.000|0.512|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.431|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class MCControlTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.3,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v1'\n",
    "                 ):\n",
    "        super(MCControlTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # build the dict of lists\n",
    "        self.returns = {}\n",
    "        for obs in range(self.obs_dim):\n",
    "            for act in range(self.action_dim):\n",
    "                self.returns[(obs, act)] = []\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "\n",
    "        # [TODO] You need to implement the epsilon-greedy policy here.\n",
    "        # hint: Just copy your codes in SARSATrainer.policy()\n",
    "        if np.random.random()<self.eps:\n",
    "            return np.random.choice(4)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs])\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        # [TODO] rollout for one episode, store data in three lists create \n",
    "        #  above.\n",
    "        # hint: we do not need to store next observation.\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(self.max_episode_length):\n",
    "            act = self.policy(obs)\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            observations.append(obs)\n",
    "            actions.append(act)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                obs = next_obs                \n",
    "\n",
    "        assert len(actions) == len(observations)\n",
    "        assert len(actions) == len(rewards)\n",
    "\n",
    "        occured_state_action_pair = set()\n",
    "        length = len(actions)\n",
    "        value = 0\n",
    "        for i in reversed(range(length)):\n",
    "            # if length = 10, then i = 9, 8, ..., 0\n",
    "\n",
    "            obs = observations[i]\n",
    "            act = actions[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "            # [TODO] compute the value reversely\n",
    "            # hint: value(t) = gamma * value(t+1) + r(t)\n",
    "            value = self.gamma*value+reward\n",
    "\n",
    "            if (obs, act) not in occured_state_action_pair:\n",
    "                occured_state_action_pair.add((obs, act))\n",
    "\n",
    "                # [TODO] append current return (value) to dict\n",
    "                # hint: `value` represents the future return due to \n",
    "                #  current (obs, act), so we need to store this value\n",
    "                #  in trainer.returns\n",
    "                self.returns[(obs, act)].append(value)\n",
    "\n",
    "                # [TODO] compute the Q value from self.returns and write it \n",
    "                #  into self.table\n",
    "                self.table[obs][act] = np.mean(self.returns[(obs, act)])\n",
    "\n",
    "                # we don't need to update the policy since it is \n",
    "                # automatically adjusted with self.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_mc_control_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def mc_control(train_config=None):\n",
    "    config = default_mc_control_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = MCControlTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.001.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.623.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.649.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.665.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.65.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.631.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.643.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.663.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.661.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.635.\n",
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.63.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.654.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.622.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.673.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.603.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.636.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.625.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.68.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.656.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.652.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.673.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.635.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.646.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.662.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.667.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.637.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.677.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.667.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer = mc_control()\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.038|0.012|0.042|0.145|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.054|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.114|0.163|0.289|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.510|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.144|0.000|0.321|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.228|0.000|0.504|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.515|0.747|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.509|0.739|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.060|0.178|0.078|0.058|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.349|0.454|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.750|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.053|0.035|0.192|0.097|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.068|0.000|0.224|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.108|0.000|0.318|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.375|0.511|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b32a455649a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnew_mc_control_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmc_control\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mnew_q_learning_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnew_sarsa_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-bf06239379d4>\u001b[0m in \u001b[0;36mmc_control\u001b[1;34m(train_config)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_iteration'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# train the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# evaluate the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-3e529bb48c0c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;31m# [TODO] compute the Q value from self.returns and write it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m#  into self.table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;31m# we don't need to update the policy since it is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 3373\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   3374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# It's ok to leave this cell commented.\n",
    "\n",
    "new_config = dict(\n",
    "    env_name=\"FrozenLake8x8-v1\"\n",
    ")\n",
    "\n",
    "new_mc_control_trainer = mc_control(new_config)\n",
    "new_q_learning_trainer = q_learning(new_config)\n",
    "new_sarsa_trainer = sarsa(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
