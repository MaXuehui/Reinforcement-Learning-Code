{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8757ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d8b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='FrozenLake8x8-v1',\n",
    "             render=False, existing_env=None):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    if existing_env is None:\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(seed)\n",
    "    else:\n",
    "        env = existing_env\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                wait(sleep=0.05)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    if render:\n",
    "        env.close()\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d672e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf0dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODOs and remove \"pass\"\n",
    "\n",
    "default_config = dict(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "        if self.env_name == \"Pong-ram-v0\":\n",
    "            self.env = wrap_deepmind_ram(self.env)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        np.random.seed(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # We set self.obs_dim to the number of possible observation\n",
    "        # if observation space is discrete, otherwise the number\n",
    "        # of observation's dimensions. The same to self.act_dim.\n",
    "        if isinstance(self.env.observation_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.observation_space.shape) == 1\n",
    "            self.obs_dim = self.env.observation_space.shape[0]\n",
    "            self.discrete_obs = False\n",
    "        elif isinstance(self.env.observation_space,\n",
    "                        gym.spaces.discrete.Discrete):\n",
    "            self.obs_dim = self.env.observation_space.n\n",
    "            self.discrete_obs = True\n",
    "        else:\n",
    "            raise ValueError(\"Wrong observation space!\")\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.action_space.shape) == 1\n",
    "            self.act_dim = self.env.action_space.shape[0]\n",
    "        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\n",
    "            self.act_dim = self.env.action_space.n\n",
    "        else:\n",
    "            raise ValueError(\"Wrong action space!\")\n",
    "\n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation).\n",
    "\n",
    "        If the environment provides discrete observation (state), transform\n",
    "        it to one-hot form. For example, the environment FrozenLake-v0\n",
    "        provides an integer in [0, ..., 15] denotes the 16 possible states.\n",
    "        We transform it to one-hot style:\n",
    "\n",
    "        original state 0 -> one-hot vector [1, 0, 0, 0, 0, 0, 0, 0, ...]\n",
    "        original state 1 -> one-hot vector [0, 1, 0, 0, 0, 0, 0, 0, ...]\n",
    "        original state 15 -> one-hot vector [0, ..., 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "        If the observation space is continuous, then you should do nothing.\n",
    "        \"\"\"\n",
    "        if not self.discrete_obs:\n",
    "            return state\n",
    "        else:\n",
    "            new_state = np.zeros((self.obs_dim,))\n",
    "            new_state[state] = 1\n",
    "        return new_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Approximate the state value of given state.\n",
    "        This is a private function.\n",
    "        Note that you should NOT preprocess the state here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # [TODO] Implement the epsilon-greedy policy here. We have `eps`\n",
    "        #  probability to choose a uniformly random action in action_space,\n",
    "        #  otherwise choose action that maximizes the values.\n",
    "        # Hint: Use the function of self.env.action_space to sample random\n",
    "        # action.\n",
    "        if np.random.uniform(0,1)<self.eps:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            return np.argmax(values)\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        if \"MetaDrive\" in self.env_name:\n",
    "            kwargs[\"existing_env\"] = self.env\n",
    "        result = evaluate(policy, num_episodes, seed=self.seed,\n",
    "                          env_name=self.env_name, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81aef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for a random policy in 500 episodes in CartPole-v0:  22.554\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestTrainer(AbstractTrainer):\n",
    "    \"\"\"This class is used for testing. We don't really train anything.\"\"\"\n",
    "    def compute_values(self, state):\n",
    "        return np.random.random_sample(size=self.act_dim)\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "    \n",
    "t = TestTrainer(dict(env_name=\"CartPole-v0\"))\n",
    "obs = t.env.observation_space.sample()\n",
    "processed = t.process_state(obs)\n",
    "assert processed.shape == (4, )\n",
    "assert np.all(processed == obs)\n",
    "# Test compute_action\n",
    "values = t.compute_values(processed)\n",
    "correct_act = np.argmax(values)\n",
    "assert t.compute_action(processed, eps=0) == correct_act\n",
    "print(\"Average episode reward for a random policy in 500 episodes in CartPole-v0: \",\n",
    "      t.evaluate(num_episodes=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba00e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "linear_approximator_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class LinearTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_approximator_config)\n",
    "\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.parameters, which is two dimensional matrix,\n",
    "        #  and subjects to a normal distribution with scale\n",
    "        #  config[\"parameter_std\"].\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.parameters = std*np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "        \n",
    "        print(\"Initialize parameters with shape: {}.\".format(\n",
    "            self.parameters.shape))\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        # [TODO] Compute the value for each potential action. Note that you\n",
    "        #  should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        \n",
    "        ret = np.dot(self.parameters.transpose(),processed_state.transpose())\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Please implement the n-step Sarsa algorithm presented in Chapter 10.2\n",
    "        of the textbook. You algorithm should reduce the convention one-step\n",
    "        Sarsa when n = 1. That is:\n",
    "            TD = r_t + gamma * Q(s_t+1, a_t+1) - Q(s_t, a_t)\n",
    "            Q(s_t, a_t) = Q(s_t, a_t) + learning_rate * TD\n",
    "        \"\"\"\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        processed_states = [processed_s]\n",
    "        rewards = [0.0]\n",
    "        actions = [self.compute_action(processed_s)]\n",
    "        T = float(\"inf\")\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            if t < T:\n",
    "                # [TODO]  When the termination is not reach, apply action,\n",
    "                #  process state, record state / reward / action to the\n",
    "                #  lists defined above, and deal with termination.\n",
    "                next_state, reward, done, info  = self.env.step(actions[t])\n",
    "                                \n",
    "                processed_s = self.process_state(next_state)\n",
    "                processed_states.append(processed_s)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    next_act = self.compute_action(processed_s)\n",
    "                    actions.append(next_act)\n",
    "\n",
    "            tau = t - self.n + 1\n",
    "            if tau >= 0:\n",
    "                gradient = self.compute_gradient(\n",
    "                    processed_states, actions, rewards, tau, T\n",
    "                )\n",
    "                self.apply_gradient(gradient)\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "\n",
    "        # [TODO] Compute the approximation goal, the truth state action value\n",
    "        #  G. It is a n-step discounted sum of rewards. Refer to Chapter 10.2\n",
    "        #  of the textbook.\n",
    "        # [HINT] G have two parts: the accumuted reward computed from step tau to \n",
    "        #  step tau+n, and the possible state value at time step tau+n, if the episode\n",
    "        #  is not terminated. Remember to apply the discounter factor (\\gamma^n) to\n",
    "        #  the second part of G if applicable.\n",
    "        G = 0\n",
    "        for t in reversed(range(n)):\n",
    "            G = (G**self.gamma)+rewards[t+tau]\n",
    "                \n",
    "        if tau + n < T:\n",
    "            # [TODO] If at time step tau + n the episode is not terminated,\n",
    "            # then we should add the state action value at tau + n\n",
    "            # to the G.\n",
    "            G += (self.gamma**n)*max(self.compute_values(processed_states[tau+n]))\n",
    "        \n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the weights can be separated into two\n",
    "        # parts (the chain rule):\n",
    "        #     dLoss / dweight = (dLoss / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predicted value (Q(s_t, a_t)) to be the loss.\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "        # [TODO] fill the propoer value of loss_grad, denoting the gradient\n",
    "        # of the MSE w.r.t. the output of the linear function.\n",
    "        loss_grad[:,0] = -(G*np.ones(2)-self.compute_values(processed_states[tau]))\n",
    "\n",
    "        # [TODO] compute the value of value_grad, denoting the gradient of\n",
    "        # the output of the linear function w.r.t. the parameters.\n",
    "        value_grad = np.zeros((self.obs_dim, 1))\n",
    "        value_grad[:,0] = processed_states[tau]\n",
    "\n",
    "        assert loss_grad.shape == (self.act_dim, 1)\n",
    "        assert value_grad.shape == (self.obs_dim, 1)\n",
    "\n",
    "        # [TODO] merge two gradients to get the gradient of loss w.r.t. the\n",
    "        # parameters.\n",
    "        gradient = np.dot(value_grad,loss_grad.transpose())\n",
    "        pass\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        # [TODO] apply the gradient to self.parameters\n",
    "        self.parameters -= self.learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e91a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "Now your codes should be bug-free.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = LinearTrainer(dict(parameter_std=0.0))\n",
    "\n",
    "# Test self.parameters.\n",
    "assert test_trainer.parameters.std() == 0.0, \\\n",
    "    \"Parameters should subjects to a normal distribution with standard \" \\\n",
    "    \"deviation config['parameter_std'], but you have {}.\" \\\n",
    "    \"\".format(test_trainer.parameters.std())\n",
    "assert test_trainer.parameters.mean() == 0, \\\n",
    "    \"Parameters should subjects to a normal distribution with mean 0. \" \\\n",
    "    \"But you have {}.\".format(test_trainer.parameters.mean())\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim, ), processed_state.shape\n",
    "values = test_trainer.compute_values(fake_state)\n",
    "assert values.shape == (test_trainer.act_dim, ), values.shape\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "# Test compute_gradient\n",
    "tmp_gradient = test_trainer.compute_gradient(\n",
    "    [processed_state]*10, [test_trainer.env.action_space.sample()]*10, [0.0]*10, 2, 5)\n",
    "assert tmp_gradient.shape == test_trainer.parameters.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad476b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "(0.2s,+0.2s)\tIteration 0, current mean episode reward is 145.16. \n",
      "(2.1s,+1.9s)\tIteration 1000, current mean episode reward is 21.94. \n",
      "(3.0s,+0.9s)\tIteration 2000, current mean episode reward is 20.86. \n",
      "(3.9s,+0.9s)\tIteration 3000, current mean episode reward is 19.14. \n",
      "(4.5s,+0.7s)\tIteration 4000, current mean episode reward is 11.42. \n",
      "(5.1s,+0.5s)\tIteration 5000, current mean episode reward is 11.5. \n",
      "(5.6s,+0.5s)\tIteration 6000, current mean episode reward is 11.84. \n",
      "(6.2s,+0.6s)\tIteration 7000, current mean episode reward is 11.12. \n",
      "(6.9s,+0.7s)\tIteration 8000, current mean episode reward is 11.44. \n",
      "(7.7s,+0.8s)\tIteration 9000, current mean episode reward is 11.78. \n",
      "(8.5s,+0.8s)\tIteration 10000, current mean episode reward is 11.28. \n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_trainer, _ = run(LinearTrainer, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "))\n",
    "\n",
    "# It's OK to see bad performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca86dceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your linear agent in CartPole-v0:  11.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your linear agent in CartPole-v0: \",\n",
    "      linear_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d95910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "import math\n",
    "\n",
    "linear_fc_config = merge_config(dict(\n",
    "    polynomial_order=1,\n",
    "), linear_approximator_config)\n",
    "\n",
    "\n",
    "def polynomial_feature(sequence, order=1):\n",
    "    \"\"\"\n",
    "    Construct the order-n polynomial-basis feature of the state.\n",
    "    Refer to Chapter 9.5.1 of the textbook. \n",
    "    We expect to get a vector of length `(order+1)^k` as the output,\n",
    "    wherein `k` is the dimensions of the state.\n",
    "\n",
    "    For example:\n",
    "    When the state is [2, 3, 4] (so k=3), \n",
    "    the first order polynomial feature of the state is \n",
    "    [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        2 * 3 = 6,\n",
    "        2 * 4 = 8,\n",
    "        3 * 4 = 12,\n",
    "        2 * 3 * 4 = 24\n",
    "    ].\n",
    "    \n",
    "    We have `(1+1)^3=8` output dimensions.\n",
    "\n",
    "    Note: it is not necessary to follow the ascending order.\n",
    "    \"\"\"\n",
    "    # [TODO] finish this function.\n",
    "    output = []\n",
    "    if len(sequence)==3:\n",
    "        for i in range(order+1):\n",
    "            temp1 = sequence[0]**i\n",
    "            for j in range(order+1):\n",
    "                temp2 = temp1*sequence[1]**j\n",
    "                for k in range(order+1):\n",
    "                    output.append(temp2*sequence[2]**k)\n",
    "    if len(sequence)==4:\n",
    "        for i in range(order+1):\n",
    "            temp1 = sequence[0]**i\n",
    "            for j in range(order+1):\n",
    "                temp2 = temp1*sequence[1]**j\n",
    "                for k in range(order+1):\n",
    "                    temp3 = temp2*sequence[2]**k\n",
    "                    for r in range(order+1):\n",
    "                        output.append(temp3*sequence[3]**r)      \n",
    "    return output\n",
    "  \n",
    "assert sorted(polynomial_feature([2, 3, 4])) == [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "assert len(polynomial_feature([2, 3, 4], 2)) == 27\n",
    "assert len(polynomial_feature([2, 3, 4], 3)) == 64\n",
    "\n",
    "class LinearTrainerWithFeatureConstruction(LinearTrainer):\n",
    "    \"\"\"In this class, we will expand the dimension of the state.\n",
    "    This procedure is done at self.process_state function.\n",
    "    The modification of self.obs_dim and the shape of parameters\n",
    "    is also needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_fc_config)\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.polynomial_order = self.config[\"polynomial_order\"]\n",
    "\n",
    "        # Expand the size of observation\n",
    "        self.obs_dim = (self.polynomial_order + 1) ** self.obs_dim\n",
    "\n",
    "        # Since we change self.obs_dim, reset the parameters.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Please finish the polynomial function.\"\"\"\n",
    "        processed = polynomial_feature(state, self.polynomial_order)\n",
    "        processed = np.asarray(processed)\n",
    "        assert len(processed) == self.obs_dim, processed.shape\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77ad83da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "Initialize parameters with shape: (16, 2).\n",
      "(0.1s,+0.1s)\tIteration 0, current mean episode reward is 10.94. \n",
      "(4.1s,+4.0s)\tIteration 1000, current mean episode reward is 26.74. \n",
      "(8.5s,+4.4s)\tIteration 2000, current mean episode reward is 34.38. \n",
      "(13.0s,+4.4s)\tIteration 3000, current mean episode reward is 20.1. \n",
      "(17.2s,+4.2s)\tIteration 4000, current mean episode reward is 19.34. \n",
      "(21.2s,+4.0s)\tIteration 5000, current mean episode reward is 19.8. \n",
      "(24.7s,+3.5s)\tIteration 6000, current mean episode reward is 19.98. \n",
      "(28.2s,+3.5s)\tIteration 7000, current mean episode reward is 21.54. \n",
      "(31.9s,+3.7s)\tIteration 8000, current mean episode reward is 20.0. \n",
      "(35.4s,+3.5s)\tIteration 9000, current mean episode reward is 18.0. \n",
      "(38.8s,+3.4s)\tIteration 10000, current mean episode reward is 22.8. \n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_fc_trainer, _ = run(LinearTrainerWithFeatureConstruction, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    polynomial_order=1,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert linear_fc_trainer.evaluate() > 20.0, \"The best episode reward happening \" \\\n",
    "    \"during training should be greater than the random baseline. That is more than 20+.\"\n",
    "\n",
    "# This cell should be finished within 10 minitines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a541843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In CartPole-v0, the average episode reward for the value estimator with feature construction is:  11.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\n",
    "    \"In CartPole-v0, the average episode reward for the value estimator \"\n",
    "    \"with feature construction is: \",\n",
    "      linear_fc_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7227a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(LinearTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.hidden_parameters and self.output_parameters,\n",
    "        #  which are two dimensional matrices, and subject to normal\n",
    "        #  distributions with scale config[\"parameter_std\"]\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = std*np.random.random_sample(size=(self.obs_dim, self.hidden_dim))\n",
    "        self.output_parameters = std*np.random.random_sample(size=(self.hidden_dim, self.act_dim))\n",
    "        \n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"[TODO] Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = np.dot(self.output_parameters.transpose(),activation)  \n",
    "        \n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        \"\"\"[TODO] Compute the action values values.\n",
    "        Given a processed state, first we need to compute the activtaion\n",
    "        (the output of hidden layer). Then we compute the values (the output of\n",
    "        the output layer).\n",
    "        \"\"\"\n",
    "        activation = np.dot(self.hidden_parameters.transpose(),processed_state)\n",
    "\n",
    "    \n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        \n",
    "        # [TODO] compute the target value.\n",
    "        # Hint: copy your codes in LinearTrainer.\n",
    "        G = 0\n",
    "        #for t in reversed(range(n)):\n",
    "            #G = (G**self.gamma)+rewards[t+tau]\n",
    "        for t in range(tau, tau+n-1):\n",
    "            G += (self.gamma**(t-tau))*rewards[t]    \n",
    "            \n",
    "                  \n",
    "        if tau + n < T:\n",
    "            G += (self.gamma**n)*max(self.compute_values(processed_states[tau+n]))\n",
    "\n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the output layer weights can be \n",
    "        # separated into two parts (the chain rule):\n",
    "        #     dError / dweight = (dError / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predict value (Q(s_t, a_t)) to be the loss.\n",
    "        #cur_state = processed_states[tau]\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        # [TODO] compute loss_grad\n",
    "        loss_grad[:,0] = -(G*np.ones(2)-self.compute_values(processed_states[tau]))\n",
    "        \n",
    "        # [TODO] compute the gradient of output layer parameters\n",
    "        value_gradient = np.zeros((self.hidden_dim, 1))\n",
    "        value_gradient[:,0] = self.compute_activation(processed_states[tau])\n",
    "        output_gradient = np.dot(value_gradient, loss_grad.transpose())\n",
    "\n",
    "        \n",
    "        # [TODO] compute the gradient of hidden layer parameters\n",
    "        # Hint: using chain rule and derive the formulation\n",
    "        cur_state = np.zeros((self.obs_dim, 1))\n",
    "        cur_state[:,0] = processed_states[tau]\n",
    "        activate_gradient = np.dot(self.output_parameters,loss_grad)      \n",
    "        hidden_gradient = np.dot(cur_state,activate_gradient.transpose()) \n",
    "    \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        # [TODO] Implement the clip gradient mechansim\n",
    "        # Hint: when the old gradient has norm less that clip_norm,\n",
    "        #  then nothing happens. Otherwise shrink the gradient to\n",
    "        #  make its norm equal to clip_norm.\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            if np.linalg.norm(output_gradient,2)>clip_norm:\n",
    "                output_gradient = output_gradient*clip_norm/np.linalg.norm(output_gradient,2)\n",
    "            if np.linalg.norm(hidden_gradient,2)>clip_norm:\n",
    "                hidden_gradient = hidden_gradient*clip_norm/np.linalg.norm(hidden_gradient,2)\n",
    "\n",
    "        # [TODO] update the parameters\n",
    "        # Hint: Remember to check the sign when applying the gradient\n",
    "        #  into the parameters. Should you add or minus the gradients?\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.hidden_parameters -= self.learning_rate*hidden_gradient\n",
    "        self.output_parameters -= self.learning_rate*output_gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eef8c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's see what happen if gradient clipping is not enable!\n",
      "\n",
      "(0.0s,+0.0s)\tIteration 0, current mean episode reward is 11.34. \n",
      "(0.4s,+0.3s)\tIteration 100, current mean episode reward is 11.74. \n",
      "(0.5s,+0.1s)\tIteration 200, current mean episode reward is 11.2. \n",
      "(0.6s,+0.1s)\tIteration 300, current mean episode reward is 11.78. \n",
      "(0.8s,+0.1s)\tIteration 400, current mean episode reward is 11.12. \n",
      "(0.9s,+0.1s)\tIteration 500, current mean episode reward is 10.6. \n",
      "(1.0s,+0.1s)\tIteration 600, current mean episode reward is 13.34. \n",
      "(1.2s,+0.1s)\tIteration 700, current mean episode reward is 11.92. \n",
      "(1.3s,+0.1s)\tIteration 800, current mean episode reward is 10.94. \n",
      "(1.6s,+0.3s)\tIteration 900, current mean episode reward is 50.04. \n",
      "(1.8s,+0.2s)\tIteration 1000, current mean episode reward is 11.02. \n",
      "(1.9s,+0.1s)\tIteration 1100, current mean episode reward is 11.32. \n",
      "(2.0s,+0.1s)\tIteration 1200, current mean episode reward is 11.04. \n",
      "(2.2s,+0.2s)\tIteration 1300, current mean episode reward is 11.38. \n",
      "(2.3s,+0.1s)\tIteration 1400, current mean episode reward is 10.66. \n",
      "(2.5s,+0.1s)\tIteration 1500, current mean episode reward is 11.1. \n",
      "(2.6s,+0.1s)\tIteration 1600, current mean episode reward is 10.72. \n",
      "(2.7s,+0.1s)\tIteration 1700, current mean episode reward is 11.1. \n",
      "(2.9s,+0.1s)\tIteration 1800, current mean episode reward is 11.66. \n",
      "(3.0s,+0.1s)\tIteration 1900, current mean episode reward is 11.46. \n",
      "(3.1s,+0.1s)\tIteration 2000, current mean episode reward is 11.26. \n",
      "(3.3s,+0.1s)\tIteration 2100, current mean episode reward is 11.42. \n",
      "(3.4s,+0.1s)\tIteration 2200, current mean episode reward is 11.28. \n",
      "(3.5s,+0.1s)\tIteration 2300, current mean episode reward is 11.66. \n",
      "(3.7s,+0.1s)\tIteration 2400, current mean episode reward is 10.92. \n",
      "(3.8s,+0.2s)\tIteration 2500, current mean episode reward is 10.7. \n",
      "(4.0s,+0.1s)\tIteration 2600, current mean episode reward is 11.0. \n",
      "(4.1s,+0.1s)\tIteration 2700, current mean episode reward is 11.44. \n",
      "(4.2s,+0.1s)\tIteration 2800, current mean episode reward is 11.16. \n",
      "(4.3s,+0.1s)\tIteration 2900, current mean episode reward is 10.78. \n",
      "(4.4s,+0.1s)\tIteration 3000, current mean episode reward is 11.3. \n",
      "\n",
      "We expect to see bad performance (<195). The performance without gradient clipping: 11.46.\n",
      "Try next cell to see the impact of gradient clipping.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if gradient clipping is not enable!\\n\")\n",
    "try:\n",
    "    failed_mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "        max_iteration=3000,\n",
    "        evaluate_interval=100, \n",
    "        parameter_std=0.01,\n",
    "        learning_rate=0.001,\n",
    "        hidden_dim=100,\n",
    "        clip_gradient=False,  # <<< Gradient clipping is OFF!\n",
    "        env_name=\"CartPole-v0\"\n",
    "    ), reward_threshold=195.0)\n",
    "    print(\"\\nWe expect to see bad performance (<195). \"\n",
    "          \"The performance without gradient clipping: {}.\"\n",
    "          \"\".format(failed_mlp_trainer.evaluate()))\n",
    "except AssertionError as e:\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Infinity happen during training. It's OK since the gradient is not bounded.\")\n",
    "finally:\n",
    "    print(\"Try next cell to see the impact of gradient clipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031d202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's see what happen if gradient clipping is enable!\n",
      "\n",
      "(0.1s,+0.1s)\tIteration 0, current mean episode reward is 11.34. \n",
      "(0.6s,+0.5s)\tIteration 100, current mean episode reward is 11.74. \n",
      "(0.8s,+0.2s)\tIteration 200, current mean episode reward is 11.2. \n",
      "(1.0s,+0.2s)\tIteration 300, current mean episode reward is 11.78. \n",
      "(1.2s,+0.2s)\tIteration 400, current mean episode reward is 11.12. \n",
      "(1.4s,+0.2s)\tIteration 500, current mean episode reward is 10.6. \n",
      "(1.6s,+0.2s)\tIteration 600, current mean episode reward is 13.34. \n",
      "(1.8s,+0.2s)\tIteration 700, current mean episode reward is 11.92. \n",
      "(2.0s,+0.2s)\tIteration 800, current mean episode reward is 10.94. \n",
      "(2.5s,+0.4s)\tIteration 900, current mean episode reward is 50.04. \n",
      "(2.7s,+0.3s)\tIteration 1000, current mean episode reward is 11.02. \n",
      "(2.9s,+0.2s)\tIteration 1100, current mean episode reward is 11.32. \n",
      "(3.1s,+0.2s)\tIteration 1200, current mean episode reward is 11.04. \n",
      "(3.3s,+0.2s)\tIteration 1300, current mean episode reward is 11.38. \n",
      "(3.5s,+0.2s)\tIteration 1400, current mean episode reward is 10.66. \n",
      "(3.7s,+0.2s)\tIteration 1500, current mean episode reward is 11.1. \n",
      "(3.9s,+0.2s)\tIteration 1600, current mean episode reward is 10.72. \n",
      "(4.1s,+0.2s)\tIteration 1700, current mean episode reward is 11.1. \n",
      "(4.3s,+0.2s)\tIteration 1800, current mean episode reward is 11.66. \n",
      "(4.4s,+0.2s)\tIteration 1900, current mean episode reward is 11.46. \n",
      "(4.6s,+0.2s)\tIteration 2000, current mean episode reward is 11.26. \n",
      "(4.8s,+0.2s)\tIteration 2100, current mean episode reward is 11.42. \n",
      "(5.1s,+0.3s)\tIteration 2200, current mean episode reward is 11.28. \n",
      "(5.3s,+0.3s)\tIteration 2300, current mean episode reward is 11.66. \n",
      "(5.6s,+0.3s)\tIteration 2400, current mean episode reward is 10.92. \n",
      "(5.8s,+0.2s)\tIteration 2500, current mean episode reward is 10.7. \n",
      "(6.1s,+0.2s)\tIteration 2600, current mean episode reward is 11.0. \n",
      "(6.3s,+0.3s)\tIteration 2700, current mean episode reward is 11.44. \n",
      "(6.6s,+0.3s)\tIteration 2800, current mean episode reward is 11.16. \n",
      "(6.9s,+0.3s)\tIteration 2900, current mean episode reward is 10.78. \n",
      "(7.1s,+0.3s)\tIteration 3000, current mean episode reward is 11.3. \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Check your codes. Your agent should achieve {} reward in 200 iterations.But it achieve {} reward in evaluation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7132\\640451662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m ), reward_threshold=195.0)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mmlp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m195.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Check your codes. \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;34m\"Your agent should achieve {} reward in 200 iterations.\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;34m\"But it achieve {} reward in evaluation.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Check your codes. Your agent should achieve {} reward in 200 iterations.But it achieve {} reward in evaluation."
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if gradient clipping is enable!\\n\")\n",
    "mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "    max_iteration=3000,\n",
    "    evaluate_interval=100, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dim=100,\n",
    "    clip_gradient=True,  # <<< Gradient clipping is ON!\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert mlp_trainer.evaluate() > 195.0, \"Check your codes. \" \\\n",
    "    \"Your agent should achieve {} reward in 200 iterations.\" \\\n",
    "    \"But it achieve {} reward in evaluation.\"\n",
    "\n",
    "# In our implementation, the task is solved in 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b83c8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d977831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    \"\"\"Store and sample the transitions\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        # deque is a useful class which acts like a list but only contain\n",
    "        # finite elements.When appending new element make deque exceeds the \n",
    "        # `maxlen`, the oldest element (the index 0 element) will be removed.\n",
    "        \n",
    "        # [TODO] uncomment next line. \n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        pass\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b8d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(PytorchModel, self).__init__()\n",
    "        \n",
    "        # [TODO] Build a sequential model with two layers.\n",
    "        # The first hidden layer has 100 hidden nodes, followed by\n",
    "        # a ReLU activation function.\n",
    "        # The second output layer take the activation vector, who has\n",
    "        # 100 elements, as input and return the action values.\n",
    "        # So the return values is a vector with num_actions elements.\n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.num_hidden = 100\n",
    "        \n",
    "        self.hidden_value = nn.Linear(self.input_shape[0], self.num_hidden)\n",
    "        self.action_value = nn.Linear(self.num_hidden, self.num_actions)\n",
    "        \n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = F.relu(self.hidden_value(obs))\n",
    "        return self.action_value(obs)\n",
    "    \n",
    "# Test\n",
    "assert isinstance(PytorchModel((3,), 7).action_value, nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "709f43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "pytorch_config = merge_config(dict(\n",
    "    memory_size=50000,\n",
    "    learn_start=5000,\n",
    "    batch_size=32,\n",
    "    target_update_freq=500,  # in steps\n",
    "    learn_freq=1,  # in steps\n",
    "    n=1\n",
    "), mlp_trainer_config)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        x = torch.from_numpy(x).type(torch.float32)\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    if x.dim() == 3 or x.dim() == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
    "    return x\n",
    "\n",
    "\n",
    "class DQNTrainer(MLPTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, pytorch_config)\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(config[\"memory_size\"])\n",
    "        self.learn_start = config[\"learn_start\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.target_update_freq = config[\"target_update_freq\"]\n",
    "        self.clip_norm = config[\"clip_norm\"]\n",
    "        self.step_since_update = 0\n",
    "        self.total_step = 0\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        input_shape = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        # [TODO] Initialize two network using PytorchModel class\n",
    "        self.network = PytorchModel(input_shape, self.num_actions)\n",
    "\n",
    "        self.network.eval()\n",
    "        self.network.share_memory()\n",
    "\n",
    "        # [TODO] Initialize target network then copy the weight\n",
    "        # of original network to it. So you should\n",
    "        # put the weights of self.network into self.target_network.\n",
    "        self.target_network = PytorchModel(input_shape, self.num_actions)\n",
    "        \n",
    "        self.target_network.eval()\n",
    "\n",
    "        # Build Adam optimizer and MSE Loss.\n",
    "        # [TODO] Uncomment next few lines\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        # [TODO] Convert the output of neural network to numpy array\n",
    "        params = list(self.network.parameters())\n",
    "        self.hidden_parameters = params[0].detach().numpy()\n",
    "        self.hidden_parameters_b = params[1].detach().numpy()\n",
    "        self.output_parameters = params[2].detach().numpy()\n",
    "        self.output_parameters_b = params[3].detach().numpy()\n",
    "        #import pdb; pdb.set_trace()                \n",
    "        activation = np.dot(self.hidden_parameters,processed_state.numpy())+self.hidden_parameters_b\n",
    "        values = np.dot(self.output_parameters,activation)+self.output_parameters_b\n",
    "    \n",
    "        return values\n",
    "\n",
    "    def train(self):\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        act = self.compute_action(processed_s)\n",
    "        stat = {\"loss\": []}\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            next_state, reward, done, _ = self.env.step(act)\n",
    "            next_processed_s = self.process_state(next_state)\n",
    "\n",
    "            # Push the transition into memory.\n",
    "            self.memory.push(\n",
    "                (processed_s, act, reward, next_processed_s, done)\n",
    "            )\n",
    "\n",
    "            processed_s = next_processed_s\n",
    "            act = self.compute_action(next_processed_s)\n",
    "            self.step_since_update += 1\n",
    "            self.total_step += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if t % self.config[\"learn_freq\"] != 0:\n",
    "                # It's not necessary to update in each step.\n",
    "                continue\n",
    "\n",
    "            if len(self.memory) < self.learn_start:\n",
    "                continue\n",
    "            elif len(self.memory) == self.learn_start:\n",
    "                print(\"Current memory contains {} transitions, \"\n",
    "                      \"start learning!\".format(self.learn_start))\n",
    "\n",
    "            batch = self.memory.sample(self.batch_size)\n",
    "\n",
    "            # Transform a batch of state / action / .. into a tensor.\n",
    "            state_batch = to_tensor(\n",
    "                np.stack([transition[0] for transition in batch])\n",
    "            )\n",
    "            action_batch = to_tensor(\n",
    "                np.stack([transition[1] for transition in batch])\n",
    "            )\n",
    "            reward_batch = to_tensor(\n",
    "                np.stack([transition[2] for transition in batch])\n",
    "            )\n",
    "            next_state_batch = torch.stack(\n",
    "                [transition[3] for transition in batch]\n",
    "            )\n",
    "            done_batch = to_tensor(\n",
    "                np.stack([transition[4] for transition in batch])\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # [TODO] Compute the values of Q in next state in batch.\n",
    "                Q_t_plus_one = self.target_network(next_state_batch).max(1)[0].detach()\n",
    "               \n",
    "                assert isinstance(Q_t_plus_one, torch.Tensor)\n",
    "                assert Q_t_plus_one.dim() == 1\n",
    "                \n",
    "                # [TODO] Compute the target value of Q in batch.\n",
    "                Q_target = reward_batch+0.99*Q_t_plus_one\n",
    "                #import pdb; pdb.set_trace() \n",
    "                #assert Q_target.shape == (self.batch_size,)\n",
    "            \n",
    "            # [TODO] Collect the Q values in batch.\n",
    "            # Hint: Remember to call self.network.train()\n",
    "            #  before you get the Q value from self.network(state_batch),\n",
    "            #  otherwise the graident will not be recorded by pytorch.\n",
    "            self.network.train()\n",
    "            #import pdb; pdb.set_trace() \n",
    "            Q_t = self.network(state_batch).gather(1, action_batch.long())\n",
    "    \n",
    "            assert Q_t.shape == Q_target.shape\n",
    "\n",
    "            # Update the network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss(input=Q_t, target=Q_target)\n",
    "            loss_value = loss.item()\n",
    "            stat['loss'].append(loss_value)\n",
    "            loss.backward()\n",
    "            \n",
    "            # [TODO] Gradient clipping. Uncomment next line\n",
    "            nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.network.eval()\n",
    "\n",
    "        if len(self.memory) >= self.learn_start and \\\n",
    "                self.step_since_update > self.target_update_freq:\n",
    "            print(\"{} steps has passed since last update. Now update the\"\n",
    "                  \" parameter of the behavior policy. Current step: {}\".format(\n",
    "                self.step_since_update, self.total_step\n",
    "            ))\n",
    "            self.step_since_update = 0\n",
    "            # [TODO] Copy the weights of self.network to self.target_network.\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "            \n",
    "            self.target_network.eval()\n",
    "            \n",
    "        return {\"loss\": np.mean(stat[\"loss\"]), \"episode_len\": t}\n",
    "\n",
    "    def process_state(self, state):\n",
    "        return torch.from_numpy(state).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7937769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\envs\\ierg5350\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "D:\\Softwares\\Anaconda\\envs\\ierg5350\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now your codes should be bug-free.\n",
      "(0.1s,+0.1s)\tIteration 0, current mean episode reward is 10.56. {'loss': nan, 'episode_len': 9.0}\n",
      "Current memory contains 100 transitions, start learning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Softwares\\Anaconda\\envs\\ierg5350\\lib\\site-packages\\torch\\autograd\\__init__.py:156: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2s,+1.1s)\tIteration 10, current mean episode reward is 11.38. {'loss': 0.1068, 'episode_len': 16.0}\n",
      "(1.5s,+0.3s)\tIteration 20, current mean episode reward is 11.5. {'loss': 0.0518, 'episode_len': 8.0}\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = DQNTrainer({})\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim, ), processed_state.shape\n",
    "values = test_trainer.compute_values(processed_state)\n",
    "assert values.shape == (test_trainer.act_dim, ), values.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")\n",
    "\n",
    "_ = run(DQNTrainer, dict(\n",
    "    max_iteration=20,\n",
    "    evaluate_interval=10, \n",
    "    learn_start=100,\n",
    "    env_name=\"CartPole-v0\",\n",
    "))\n",
    "\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27cc3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0s,+0.0s)\tIteration 0, current mean episode reward is 9.78. {'loss': nan, 'episode_len': 9.0}\n",
      "(0.1s,+0.0s)\tIteration 10, current mean episode reward is 9.76. {'loss': nan, 'episode_len': 7.0}\n",
      "(0.1s,+0.0s)\tIteration 20, current mean episode reward is 9.88. {'loss': nan, 'episode_len': 8.0}\n",
      "(0.2s,+0.0s)\tIteration 30, current mean episode reward is 9.6. {'loss': nan, 'episode_len': 10.0}\n",
      "(0.2s,+0.0s)\tIteration 40, current mean episode reward is 9.74. {'loss': nan, 'episode_len': 10.0}\n",
      "(0.3s,+0.1s)\tIteration 50, current mean episode reward is 9.62. {'loss': nan, 'episode_len': 9.0}\n",
      "(0.3s,+0.0s)\tIteration 60, current mean episode reward is 9.64. {'loss': nan, 'episode_len': 8.0}\n",
      "(0.4s,+0.0s)\tIteration 70, current mean episode reward is 9.6. {'loss': nan, 'episode_len': 9.0}\n",
      "(0.4s,+0.0s)\tIteration 80, current mean episode reward is 9.98. {'loss': nan, 'episode_len': 8.0}\n",
      "(0.5s,+0.1s)\tIteration 90, current mean episode reward is 9.9. {'loss': nan, 'episode_len': 8.0}\n",
      "(0.5s,+0.1s)\tIteration 100, current mean episode reward is 9.58. {'loss': nan, 'episode_len': 9.0}\n",
      "Current memory contains 1000 transitions, start learning!\n",
      "1009 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 1009\n",
      "(0.8s,+0.2s)\tIteration 110, current mean episode reward is 9.78. {'loss': 0.2209, 'episode_len': 9.0}\n",
      "(1.0s,+0.2s)\tIteration 120, current mean episode reward is 9.6. {'loss': 0.2126, 'episode_len': 7.0}\n",
      "(1.2s,+0.2s)\tIteration 130, current mean episode reward is 9.98. {'loss': 0.2561, 'episode_len': 12.0}\n",
      "(1.5s,+0.2s)\tIteration 140, current mean episode reward is 10.18. {'loss': 0.1986, 'episode_len': 9.0}\n",
      "(1.7s,+0.2s)\tIteration 150, current mean episode reward is 9.96. {'loss': 0.2067, 'episode_len': 9.0}\n",
      "(1.9s,+0.2s)\tIteration 160, current mean episode reward is 9.66. {'loss': 0.2185, 'episode_len': 8.0}\n",
      "(2.2s,+0.2s)\tIteration 170, current mean episode reward is 9.82. {'loss': 0.2201, 'episode_len': 8.0}\n",
      "(2.4s,+0.2s)\tIteration 180, current mean episode reward is 9.82. {'loss': 0.2488, 'episode_len': 9.0}\n",
      "(2.6s,+0.2s)\tIteration 190, current mean episode reward is 9.78. {'loss': 0.3233, 'episode_len': 9.0}\n",
      "(2.9s,+0.2s)\tIteration 200, current mean episode reward is 9.66. {'loss': 0.2154, 'episode_len': 8.0}\n",
      "1005 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 2014\n",
      "(3.1s,+0.2s)\tIteration 210, current mean episode reward is 9.66. {'loss': 0.0065, 'episode_len': 9.0}\n",
      "(3.3s,+0.2s)\tIteration 220, current mean episode reward is 9.88. {'loss': 0.4506, 'episode_len': 8.0}\n",
      "(3.5s,+0.2s)\tIteration 230, current mean episode reward is 10.04. {'loss': 0.2037, 'episode_len': 8.0}\n",
      "(3.8s,+0.3s)\tIteration 240, current mean episode reward is 9.46. {'loss': 0.0091, 'episode_len': 11.0}\n",
      "(4.1s,+0.2s)\tIteration 250, current mean episode reward is 9.94. {'loss': 0.0031, 'episode_len': 7.0}\n",
      "(4.3s,+0.2s)\tIteration 260, current mean episode reward is 9.96. {'loss': 0.0051, 'episode_len': 8.0}\n",
      "(4.5s,+0.2s)\tIteration 270, current mean episode reward is 9.74. {'loss': 0.0022, 'episode_len': 7.0}\n",
      "(4.7s,+0.2s)\tIteration 280, current mean episode reward is 9.64. {'loss': 0.0024, 'episode_len': 9.0}\n",
      "(5.0s,+0.2s)\tIteration 290, current mean episode reward is 9.76. {'loss': 0.0034, 'episode_len': 9.0}\n",
      "1010 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 3024\n",
      "(5.2s,+0.3s)\tIteration 300, current mean episode reward is 10.06. {'loss': 0.0083, 'episode_len': 12.0}\n",
      "(5.4s,+0.2s)\tIteration 310, current mean episode reward is 9.98. {'loss': 0.0041, 'episode_len': 10.0}\n",
      "(5.7s,+0.2s)\tIteration 320, current mean episode reward is 10.12. {'loss': 0.0019, 'episode_len': 9.0}\n",
      "(5.9s,+0.2s)\tIteration 330, current mean episode reward is 9.8. {'loss': 0.0043, 'episode_len': 8.0}\n",
      "(6.1s,+0.2s)\tIteration 340, current mean episode reward is 10.14. {'loss': 0.0027, 'episode_len': 9.0}\n",
      "(6.3s,+0.2s)\tIteration 350, current mean episode reward is 10.12. {'loss': 0.0012, 'episode_len': 8.0}\n",
      "(6.5s,+0.2s)\tIteration 360, current mean episode reward is 9.94. {'loss': 0.0043, 'episode_len': 8.0}\n",
      "(6.8s,+0.2s)\tIteration 370, current mean episode reward is 10.1. {'loss': 0.0016, 'episode_len': 8.0}\n",
      "(7.0s,+0.2s)\tIteration 380, current mean episode reward is 10.18. {'loss': 0.0008, 'episode_len': 9.0}\n",
      "(7.2s,+0.2s)\tIteration 390, current mean episode reward is 9.98. {'loss': 0.0009, 'episode_len': 8.0}\n",
      "1007 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 4031\n",
      "(7.6s,+0.4s)\tIteration 400, current mean episode reward is 9.8. {'loss': 0.0991, 'episode_len': 29.0}\n",
      "(7.8s,+0.2s)\tIteration 410, current mean episode reward is 9.88. {'loss': 0.0038, 'episode_len': 8.0}\n",
      "(8.0s,+0.2s)\tIteration 420, current mean episode reward is 9.88. {'loss': 0.0014, 'episode_len': 9.0}\n",
      "(8.3s,+0.3s)\tIteration 430, current mean episode reward is 10.18. {'loss': 0.0018, 'episode_len': 9.0}\n",
      "(8.6s,+0.2s)\tIteration 440, current mean episode reward is 9.86. {'loss': 0.0012, 'episode_len': 7.0}\n",
      "(8.8s,+0.3s)\tIteration 450, current mean episode reward is 10.24. {'loss': 0.0013, 'episode_len': 8.0}\n",
      "(9.1s,+0.3s)\tIteration 460, current mean episode reward is 10.32. {'loss': 0.001, 'episode_len': 8.0}\n",
      "(9.4s,+0.3s)\tIteration 470, current mean episode reward is 10.32. {'loss': 0.0015, 'episode_len': 9.0}\n",
      "(9.7s,+0.3s)\tIteration 480, current mean episode reward is 10.12. {'loss': 0.0017, 'episode_len': 11.0}\n",
      "1002 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 5033\n",
      "(9.9s,+0.2s)\tIteration 490, current mean episode reward is 10.26. {'loss': 0.0016, 'episode_len': 10.0}\n",
      "(10.2s,+0.3s)\tIteration 500, current mean episode reward is 9.98. {'loss': 0.0017, 'episode_len': 15.0}\n",
      "(10.4s,+0.2s)\tIteration 510, current mean episode reward is 10.44. {'loss': 0.0016, 'episode_len': 8.0}\n",
      "(10.6s,+0.2s)\tIteration 520, current mean episode reward is 11.26. {'loss': 0.0003, 'episode_len': 9.0}\n",
      "(10.9s,+0.3s)\tIteration 530, current mean episode reward is 10.3. {'loss': 0.0011, 'episode_len': 9.0}\n",
      "(11.1s,+0.3s)\tIteration 540, current mean episode reward is 10.56. {'loss': 0.0023, 'episode_len': 8.0}\n",
      "(11.4s,+0.2s)\tIteration 550, current mean episode reward is 9.92. {'loss': 0.0003, 'episode_len': 12.0}\n",
      "(11.6s,+0.2s)\tIteration 560, current mean episode reward is 10.74. {'loss': 0.0004, 'episode_len': 8.0}\n",
      "(11.8s,+0.2s)\tIteration 570, current mean episode reward is 10.12. {'loss': 0.0039, 'episode_len': 8.0}\n",
      "1002 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 6035\n",
      "(12.1s,+0.3s)\tIteration 580, current mean episode reward is 9.84. {'loss': 0.5678, 'episode_len': 9.0}\n",
      "(12.3s,+0.2s)\tIteration 590, current mean episode reward is 10.04. {'loss': 0.0163, 'episode_len': 9.0}\n",
      "(12.5s,+0.2s)\tIteration 600, current mean episode reward is 10.58. {'loss': 0.0011, 'episode_len': 9.0}\n",
      "(12.7s,+0.2s)\tIteration 610, current mean episode reward is 10.76. {'loss': 0.0004, 'episode_len': 10.0}\n",
      "(13.0s,+0.3s)\tIteration 620, current mean episode reward is 14.56. {'loss': 0.0026, 'episode_len': 20.0}\n",
      "(13.3s,+0.3s)\tIteration 630, current mean episode reward is 10.92. {'loss': 0.0003, 'episode_len': 10.0}\n",
      "(13.6s,+0.3s)\tIteration 640, current mean episode reward is 14.8. {'loss': 0.0004, 'episode_len': 15.0}\n",
      "(13.8s,+0.2s)\tIteration 650, current mean episode reward is 10.24. {'loss': 0.0002, 'episode_len': 11.0}\n",
      "(14.1s,+0.2s)\tIteration 660, current mean episode reward is 11.56. {'loss': 0.0001, 'episode_len': 9.0}\n",
      "1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 7036\n",
      "(14.3s,+0.3s)\tIteration 670, current mean episode reward is 14.76. {'loss': 0.8404, 'episode_len': 8.0}\n",
      "(14.6s,+0.3s)\tIteration 680, current mean episode reward is 11.48. {'loss': 0.0022, 'episode_len': 11.0}\n",
      "(14.8s,+0.2s)\tIteration 690, current mean episode reward is 11.3. {'loss': 0.0017, 'episode_len': 9.0}\n",
      "(15.1s,+0.3s)\tIteration 700, current mean episode reward is 9.94. {'loss': 0.0103, 'episode_len': 9.0}\n",
      "(15.3s,+0.2s)\tIteration 710, current mean episode reward is 9.98. {'loss': 0.0009, 'episode_len': 9.0}\n",
      "(15.6s,+0.3s)\tIteration 720, current mean episode reward is 10.0. {'loss': 0.0004, 'episode_len': 7.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15.8s,+0.3s)\tIteration 730, current mean episode reward is 9.84. {'loss': 0.0081, 'episode_len': 9.0}\n",
      "(16.1s,+0.3s)\tIteration 740, current mean episode reward is 9.9. {'loss': 0.001, 'episode_len': 9.0}\n",
      "(16.4s,+0.3s)\tIteration 750, current mean episode reward is 9.9. {'loss': 0.0014, 'episode_len': 10.0}\n",
      "1013 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 8049\n",
      "(16.7s,+0.3s)\tIteration 760, current mean episode reward is 9.86. {'loss': 0.5402, 'episode_len': 24.0}\n",
      "(16.9s,+0.3s)\tIteration 770, current mean episode reward is 9.88. {'loss': 0.0172, 'episode_len': 8.0}\n",
      "(17.1s,+0.2s)\tIteration 780, current mean episode reward is 10.1. {'loss': 0.0188, 'episode_len': 10.0}\n",
      "(17.4s,+0.3s)\tIteration 790, current mean episode reward is 9.9. {'loss': 0.0182, 'episode_len': 12.0}\n",
      "(17.7s,+0.3s)\tIteration 800, current mean episode reward is 10.12. {'loss': 0.017, 'episode_len': 8.0}\n",
      "(17.9s,+0.3s)\tIteration 810, current mean episode reward is 10.14. {'loss': 0.0191, 'episode_len': 9.0}\n",
      "(18.2s,+0.3s)\tIteration 820, current mean episode reward is 10.12. {'loss': 0.0673, 'episode_len': 9.0}\n",
      "(18.5s,+0.3s)\tIteration 830, current mean episode reward is 9.78. {'loss': 0.0168, 'episode_len': 8.0}\n",
      "(18.7s,+0.3s)\tIteration 840, current mean episode reward is 10.02. {'loss': 0.0139, 'episode_len': 9.0}\n",
      "(19.0s,+0.3s)\tIteration 850, current mean episode reward is 10.04. {'loss': 0.0148, 'episode_len': 9.0}\n",
      "1007 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 9056\n",
      "(19.3s,+0.2s)\tIteration 860, current mean episode reward is 9.98. {'loss': 0.1472, 'episode_len': 8.0}\n",
      "(19.6s,+0.3s)\tIteration 870, current mean episode reward is 11.88. {'loss': 0.0098, 'episode_len': 9.0}\n",
      "(20.1s,+0.5s)\tIteration 880, current mean episode reward is 25.22. {'loss': 0.001, 'episode_len': 14.0}\n",
      "(20.5s,+0.5s)\tIteration 890, current mean episode reward is 33.02. {'loss': 0.0066, 'episode_len': 11.0}\n",
      "(20.9s,+0.4s)\tIteration 900, current mean episode reward is 15.2. {'loss': 0.0017, 'episode_len': 11.0}\n",
      "(21.3s,+0.4s)\tIteration 910, current mean episode reward is 14.02. {'loss': 0.0013, 'episode_len': 14.0}\n",
      "1024 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 10080\n",
      "(21.8s,+0.4s)\tIteration 920, current mean episode reward is 16.28. {'loss': 0.0288, 'episode_len': 15.0}\n",
      "(22.1s,+0.4s)\tIteration 930, current mean episode reward is 10.52. {'loss': 0.0036, 'episode_len': 18.0}\n",
      "(22.4s,+0.3s)\tIteration 940, current mean episode reward is 11.0. {'loss': 0.0015, 'episode_len': 8.0}\n",
      "(22.6s,+0.2s)\tIteration 950, current mean episode reward is 11.8. {'loss': 0.0005, 'episode_len': 7.0}\n",
      "(22.9s,+0.3s)\tIteration 960, current mean episode reward is 11.36. {'loss': 0.0009, 'episode_len': 13.0}\n",
      "(23.2s,+0.3s)\tIteration 970, current mean episode reward is 11.36. {'loss': 0.0026, 'episode_len': 18.0}\n",
      "(23.5s,+0.4s)\tIteration 980, current mean episode reward is 12.24. {'loss': 0.0004, 'episode_len': 14.0}\n",
      "1008 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 11088\n",
      "(24.0s,+0.5s)\tIteration 990, current mean episode reward is 31.14. {'loss': 0.0047, 'episode_len': 37.0}\n",
      "(24.7s,+0.6s)\tIteration 1000, current mean episode reward is 26.94. {'loss': 0.0006, 'episode_len': 18.0}\n",
      "(25.5s,+0.8s)\tIteration 1010, current mean episode reward is 31.52. {'loss': 0.0001, 'episode_len': 40.0}\n",
      "1012 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 12100\n",
      "(26.2s,+0.7s)\tIteration 1020, current mean episode reward is 33.98. {'loss': 0.451, 'episode_len': 11.0}\n",
      "(26.9s,+0.7s)\tIteration 1030, current mean episode reward is 28.9. {'loss': 0.0007, 'episode_len': 18.0}\n",
      "(27.7s,+0.8s)\tIteration 1040, current mean episode reward is 28.08. {'loss': 0.0, 'episode_len': 24.0}\n",
      "(28.3s,+0.6s)\tIteration 1050, current mean episode reward is 27.82. {'loss': 0.0, 'episode_len': 36.0}\n",
      "1016 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 13116\n",
      "(28.6s,+0.3s)\tIteration 1060, current mean episode reward is 10.6. {'loss': 0.0049, 'episode_len': 11.0}\n",
      "(28.8s,+0.2s)\tIteration 1070, current mean episode reward is 11.34. {'loss': 0.0118, 'episode_len': 11.0}\n",
      "(29.1s,+0.3s)\tIteration 1080, current mean episode reward is 10.1. {'loss': 0.0043, 'episode_len': 12.0}\n",
      "(29.3s,+0.2s)\tIteration 1090, current mean episode reward is 9.9. {'loss': 0.0004, 'episode_len': 8.0}\n",
      "(29.5s,+0.2s)\tIteration 1100, current mean episode reward is 10.28. {'loss': 0.0004, 'episode_len': 12.0}\n",
      "(29.8s,+0.2s)\tIteration 1110, current mean episode reward is 10.04. {'loss': 0.0002, 'episode_len': 9.0}\n",
      "(30.0s,+0.2s)\tIteration 1120, current mean episode reward is 10.26. {'loss': 0.0002, 'episode_len': 8.0}\n",
      "(30.2s,+0.2s)\tIteration 1130, current mean episode reward is 10.36. {'loss': 0.0001, 'episode_len': 9.0}\n",
      "(30.5s,+0.3s)\tIteration 1140, current mean episode reward is 9.94. {'loss': 0.0001, 'episode_len': 8.0}\n",
      "1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 14117\n",
      "(30.7s,+0.2s)\tIteration 1150, current mean episode reward is 10.04. {'loss': 0.3882, 'episode_len': 10.0}\n",
      "(31.1s,+0.4s)\tIteration 1160, current mean episode reward is 24.08. {'loss': 0.0086, 'episode_len': 12.0}\n",
      "(31.6s,+0.5s)\tIteration 1170, current mean episode reward is 19.62. {'loss': 0.0005, 'episode_len': 23.0}\n",
      "(32.1s,+0.5s)\tIteration 1180, current mean episode reward is 21.52. {'loss': 0.0001, 'episode_len': 24.0}\n",
      "(32.6s,+0.5s)\tIteration 1190, current mean episode reward is 20.16. {'loss': 0.0001, 'episode_len': 20.0}\n",
      "1006 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 15123\n",
      "(32.8s,+0.2s)\tIteration 1200, current mean episode reward is 9.82. {'loss': 0.0001, 'episode_len': 8.0}\n",
      "(33.1s,+0.3s)\tIteration 1210, current mean episode reward is 10.08. {'loss': 0.0042, 'episode_len': 10.0}\n",
      "(33.3s,+0.2s)\tIteration 1220, current mean episode reward is 9.74. {'loss': 0.0034, 'episode_len': 9.0}\n",
      "(33.6s,+0.2s)\tIteration 1230, current mean episode reward is 9.64. {'loss': 0.002, 'episode_len': 10.0}\n",
      "(33.8s,+0.2s)\tIteration 1240, current mean episode reward is 9.88. {'loss': 0.0054, 'episode_len': 10.0}\n",
      "(34.1s,+0.2s)\tIteration 1250, current mean episode reward is 9.88. {'loss': 0.0062, 'episode_len': 10.0}\n",
      "(34.4s,+0.3s)\tIteration 1260, current mean episode reward is 12.44. {'loss': 0.0309, 'episode_len': 15.0}\n",
      "(34.6s,+0.3s)\tIteration 1270, current mean episode reward is 10.16. {'loss': 0.0225, 'episode_len': 8.0}\n",
      "(34.8s,+0.2s)\tIteration 1280, current mean episode reward is 10.24. {'loss': 0.0018, 'episode_len': 9.0}\n",
      "1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 16124\n",
      "(35.1s,+0.2s)\tIteration 1290, current mean episode reward is 9.7. {'loss': 0.3467, 'episode_len': 9.0}\n",
      "(35.3s,+0.3s)\tIteration 1300, current mean episode reward is 10.9. {'loss': 0.0264, 'episode_len': 8.0}\n",
      "(35.6s,+0.2s)\tIteration 1310, current mean episode reward is 9.84. {'loss': 0.0057, 'episode_len': 8.0}\n",
      "(35.9s,+0.3s)\tIteration 1320, current mean episode reward is 9.96. {'loss': 0.0077, 'episode_len': 8.0}\n",
      "(36.1s,+0.3s)\tIteration 1330, current mean episode reward is 10.04. {'loss': 0.0012, 'episode_len': 8.0}\n",
      "(36.4s,+0.2s)\tIteration 1340, current mean episode reward is 10.0. {'loss': 0.002, 'episode_len': 10.0}\n",
      "(36.6s,+0.2s)\tIteration 1350, current mean episode reward is 9.92. {'loss': 0.0118, 'episode_len': 9.0}\n",
      "(36.9s,+0.3s)\tIteration 1360, current mean episode reward is 13.32. {'loss': 0.0065, 'episode_len': 8.0}\n",
      "(37.2s,+0.3s)\tIteration 1370, current mean episode reward is 9.8. {'loss': 0.0003, 'episode_len': 10.0}\n",
      "(37.4s,+0.3s)\tIteration 1380, current mean episode reward is 9.9. {'loss': 0.0243, 'episode_len': 12.0}\n",
      "1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 17125\n",
      "(37.7s,+0.3s)\tIteration 1390, current mean episode reward is 10.08. {'loss': 0.1389, 'episode_len': 13.0}\n",
      "(37.9s,+0.2s)\tIteration 1400, current mean episode reward is 10.04. {'loss': 0.083, 'episode_len': 7.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38.2s,+0.2s)\tIteration 1410, current mean episode reward is 10.06. {'loss': 0.0293, 'episode_len': 9.0}\n",
      "(38.4s,+0.2s)\tIteration 1420, current mean episode reward is 10.06. {'loss': 0.0027, 'episode_len': 9.0}\n",
      "(38.6s,+0.2s)\tIteration 1430, current mean episode reward is 10.02. {'loss': 0.0018, 'episode_len': 8.0}\n",
      "(38.8s,+0.2s)\tIteration 1440, current mean episode reward is 10.14. {'loss': 0.002, 'episode_len': 8.0}\n",
      "(39.1s,+0.2s)\tIteration 1450, current mean episode reward is 9.82. {'loss': 0.0053, 'episode_len': 9.0}\n",
      "(39.3s,+0.3s)\tIteration 1460, current mean episode reward is 9.86. {'loss': 0.0025, 'episode_len': 11.0}\n",
      "(39.6s,+0.2s)\tIteration 1470, current mean episode reward is 10.12. {'loss': 0.0025, 'episode_len': 11.0}\n",
      "(39.8s,+0.2s)\tIteration 1480, current mean episode reward is 10.06. {'loss': 0.0019, 'episode_len': 10.0}\n",
      "(40.0s,+0.2s)\tIteration 1490, current mean episode reward is 9.86. {'loss': 0.0037, 'episode_len': 9.0}\n",
      "1002 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 18127\n",
      "(40.3s,+0.3s)\tIteration 1500, current mean episode reward is 9.82. {'loss': 0.1866, 'episode_len': 10.0}\n",
      "(40.6s,+0.2s)\tIteration 1510, current mean episode reward is 10.04. {'loss': 0.0013, 'episode_len': 7.0}\n",
      "(40.8s,+0.2s)\tIteration 1520, current mean episode reward is 10.16. {'loss': 0.0081, 'episode_len': 9.0}\n",
      "(41.1s,+0.3s)\tIteration 1530, current mean episode reward is 10.04. {'loss': 0.0152, 'episode_len': 9.0}\n",
      "(41.4s,+0.3s)\tIteration 1540, current mean episode reward is 10.2. {'loss': 0.2168, 'episode_len': 12.0}\n",
      "(41.6s,+0.3s)\tIteration 1550, current mean episode reward is 10.04. {'loss': 0.0152, 'episode_len': 9.0}\n",
      "(41.9s,+0.3s)\tIteration 1560, current mean episode reward is 9.96. {'loss': 0.0009, 'episode_len': 9.0}\n",
      "(42.1s,+0.3s)\tIteration 1570, current mean episode reward is 10.12. {'loss': 0.0018, 'episode_len': 9.0}\n",
      "(42.4s,+0.3s)\tIteration 1580, current mean episode reward is 10.34. {'loss': 0.0006, 'episode_len': 7.0}\n",
      "(42.7s,+0.3s)\tIteration 1590, current mean episode reward is 10.02. {'loss': 0.0008, 'episode_len': 9.0}\n",
      "1004 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 19131\n",
      "(43.0s,+0.3s)\tIteration 1600, current mean episode reward is 9.94. {'loss': 0.3646, 'episode_len': 8.0}\n",
      "(43.3s,+0.3s)\tIteration 1610, current mean episode reward is 10.12. {'loss': 0.011, 'episode_len': 10.0}\n",
      "(43.6s,+0.3s)\tIteration 1620, current mean episode reward is 9.98. {'loss': 0.0066, 'episode_len': 8.0}\n",
      "(43.9s,+0.3s)\tIteration 1630, current mean episode reward is 9.94. {'loss': 0.0002, 'episode_len': 10.0}\n",
      "(44.1s,+0.3s)\tIteration 1640, current mean episode reward is 10.0. {'loss': 0.0002, 'episode_len': 11.0}\n",
      "(44.4s,+0.3s)\tIteration 1650, current mean episode reward is 10.02. {'loss': 0.0001, 'episode_len': 9.0}\n",
      "(44.7s,+0.3s)\tIteration 1660, current mean episode reward is 10.0. {'loss': 0.0005, 'episode_len': 8.0}\n",
      "(44.9s,+0.2s)\tIteration 1670, current mean episode reward is 9.98. {'loss': 0.0014, 'episode_len': 10.0}\n",
      "(45.2s,+0.3s)\tIteration 1680, current mean episode reward is 9.8. {'loss': 0.0129, 'episode_len': 9.0}\n",
      "(45.5s,+0.3s)\tIteration 1690, current mean episode reward is 9.9. {'loss': 0.0035, 'episode_len': 9.0}\n",
      "1003 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 20134\n",
      "(45.7s,+0.3s)\tIteration 1700, current mean episode reward is 9.9. {'loss': 0.1622, 'episode_len': 9.0}\n",
      "(46.0s,+0.3s)\tIteration 1710, current mean episode reward is 10.06. {'loss': 0.0081, 'episode_len': 10.0}\n",
      "(46.3s,+0.3s)\tIteration 1720, current mean episode reward is 10.16. {'loss': 0.0096, 'episode_len': 8.0}\n",
      "(46.5s,+0.3s)\tIteration 1730, current mean episode reward is 9.94. {'loss': 0.0086, 'episode_len': 8.0}\n",
      "(46.8s,+0.3s)\tIteration 1740, current mean episode reward is 9.78. {'loss': 0.0301, 'episode_len': 12.0}\n",
      "(47.1s,+0.3s)\tIteration 1750, current mean episode reward is 10.06. {'loss': 0.0536, 'episode_len': 8.0}\n",
      "(47.3s,+0.2s)\tIteration 1760, current mean episode reward is 10.12. {'loss': 0.0145, 'episode_len': 10.0}\n",
      "(47.6s,+0.3s)\tIteration 1770, current mean episode reward is 10.24. {'loss': 0.0071, 'episode_len': 9.0}\n",
      "(47.9s,+0.3s)\tIteration 1780, current mean episode reward is 10.26. {'loss': 0.0081, 'episode_len': 8.0}\n",
      "(48.1s,+0.3s)\tIteration 1790, current mean episode reward is 9.92. {'loss': 0.0623, 'episode_len': 10.0}\n",
      "1001 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 21135\n",
      "(48.4s,+0.3s)\tIteration 1800, current mean episode reward is 9.94. {'loss': 0.0559, 'episode_len': 7.0}\n",
      "(48.7s,+0.3s)\tIteration 1810, current mean episode reward is 9.88. {'loss': 0.0059, 'episode_len': 10.0}\n",
      "(49.0s,+0.3s)\tIteration 1820, current mean episode reward is 10.24. {'loss': 0.0023, 'episode_len': 8.0}\n",
      "(49.3s,+0.3s)\tIteration 1830, current mean episode reward is 9.98. {'loss': 0.0385, 'episode_len': 8.0}\n",
      "(49.5s,+0.2s)\tIteration 1840, current mean episode reward is 10.1. {'loss': 0.0762, 'episode_len': 9.0}\n",
      "(49.7s,+0.2s)\tIteration 1850, current mean episode reward is 10.12. {'loss': 0.0035, 'episode_len': 9.0}\n",
      "(50.0s,+0.3s)\tIteration 1860, current mean episode reward is 10.16. {'loss': 0.0493, 'episode_len': 7.0}\n",
      "(50.3s,+0.3s)\tIteration 1870, current mean episode reward is 10.32. {'loss': 0.0345, 'episode_len': 8.0}\n",
      "(50.6s,+0.3s)\tIteration 1880, current mean episode reward is 9.9. {'loss': 0.0012, 'episode_len': 9.0}\n",
      "(50.8s,+0.3s)\tIteration 1890, current mean episode reward is 9.94. {'loss': 0.0011, 'episode_len': 8.0}\n",
      "1004 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 22139\n",
      "(51.1s,+0.3s)\tIteration 1900, current mean episode reward is 9.94. {'loss': 0.0306, 'episode_len': 8.0}\n",
      "(51.3s,+0.2s)\tIteration 1910, current mean episode reward is 9.96. {'loss': 0.0087, 'episode_len': 8.0}\n",
      "(51.6s,+0.3s)\tIteration 1920, current mean episode reward is 9.88. {'loss': 0.0004, 'episode_len': 9.0}\n",
      "(51.9s,+0.3s)\tIteration 1930, current mean episode reward is 10.02. {'loss': 0.0002, 'episode_len': 9.0}\n",
      "(52.1s,+0.3s)\tIteration 1940, current mean episode reward is 10.1. {'loss': 0.0045, 'episode_len': 9.0}\n",
      "(52.4s,+0.2s)\tIteration 1950, current mean episode reward is 10.26. {'loss': 0.0008, 'episode_len': 8.0}\n",
      "(52.6s,+0.3s)\tIteration 1960, current mean episode reward is 9.8. {'loss': 0.0001, 'episode_len': 13.0}\n",
      "(52.9s,+0.3s)\tIteration 1970, current mean episode reward is 9.84. {'loss': 0.0008, 'episode_len': 9.0}\n",
      "(53.2s,+0.3s)\tIteration 1980, current mean episode reward is 9.9. {'loss': 0.0087, 'episode_len': 10.0}\n",
      "(53.4s,+0.3s)\tIteration 1990, current mean episode reward is 9.98. {'loss': 0.1364, 'episode_len': 11.0}\n",
      "1007 steps has passed since last update. Now update the parameter of the behavior policy. Current step: 23146\n",
      "(53.7s,+0.3s)\tIteration 2000, current mean episode reward is 10.06. {'loss': 0.0037, 'episode_len': 9.0}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Check your codes. Your agent should achieve 195.0 reward within 1000 iterations.But it achieve 10.04 reward in evaluation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7132\\4184527208.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m195.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Check your codes. \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;34m\"Your agent should achieve {} reward within 1000 iterations.\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;34m\"But it achieve {} reward in evaluation.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m195.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Should solve the task in 10 minutes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Check your codes. Your agent should achieve 195.0 reward within 1000 iterations.But it achieve 10.04 reward in evaluation."
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pytorch_trainer, pytorch_stat = run(DQNTrainer, dict(\n",
    "    max_iteration=2000,\n",
    "    evaluate_interval=10, \n",
    "    learning_rate=0.01,\n",
    "    clip_norm=10.0,\n",
    "    memory_size=50000,\n",
    "    learn_start=1000,\n",
    "    eps=0.1,\n",
    "    target_update_freq=1000,\n",
    "    batch_size=32,\n",
    "    env_name=\"CartPole-v0\",\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "reward = pytorch_trainer.evaluate()\n",
    "assert reward > 195.0, \"Check your codes. \" \\\n",
    "    \"Your agent should achieve {} reward within 1000 iterations.\" \\\n",
    "    \"But it achieve {} reward in evaluation.\".format(195.0, reward)\n",
    "\n",
    "# Should solve the task in 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e249fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your Pytorch agent in CartPole-v0:  11.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Pytorch agent in CartPole-v0: \",\n",
    "      pytorch_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7280f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
