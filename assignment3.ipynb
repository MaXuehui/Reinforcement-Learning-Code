{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "#import torch\n",
    "from utils import *\n",
    "#import torch\n",
    "#import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def evaluate(policy, num_episodes=1, seed=0, env_name='FrozenLake8x8-v1',\n",
    "             render=False, existing_env=None):\n",
    "    \"\"\"This function evaluate the given policy and return the mean episode \n",
    "    reward.\n",
    "    :param policy: a function whose input is the observation\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: the random seed\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag indicating whether to render policy\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "    if existing_env is None:\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(seed)\n",
    "    else:\n",
    "        env = existing_env\n",
    "    rewards = []\n",
    "    if render: num_episodes = 1\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            act = policy(obs)\n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "                wait(sleep=0.05)\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    if render:\n",
    "        env.close()\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config.get(\"evaluate_num_episodes\", 50))\n",
    "            print(\"({:.1f}s,+{:.1f}s)\\tIteration {}, current mean episode \"\n",
    "                  \"reward is {}. {}\".format(\n",
    "                time.time() - start, time.time() - now, i, reward,\n",
    "                {k: round(np.mean(v), 4) for k, v in\n",
    "                 stat.items()} if stat else \"\"))\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    return trainer, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODOs and remove \"pass\"\n",
    "\n",
    "default_config = dict(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    evaluate_interval=100,\n",
    "    gamma=0.99,\n",
    "    eps=0.3,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = merge_config(config, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "        if self.env_name == \"Pong-ram-v0\":\n",
    "            self.env = wrap_deepmind_ram(self.env)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        np.random.seed(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # We set self.obs_dim to the number of possible observation\n",
    "        # if observation space is discrete, otherwise the number\n",
    "        # of observation's dimensions. The same to self.act_dim.\n",
    "        if isinstance(self.env.observation_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.observation_space.shape) == 1\n",
    "            self.obs_dim = self.env.observation_space.shape[0]\n",
    "            self.discrete_obs = False\n",
    "        elif isinstance(self.env.observation_space,\n",
    "                        gym.spaces.discrete.Discrete):\n",
    "            self.obs_dim = self.env.observation_space.n\n",
    "            self.discrete_obs = True\n",
    "        else:\n",
    "            raise ValueError(\"Wrong observation space!\")\n",
    "\n",
    "        if isinstance(self.env.action_space, gym.spaces.box.Box):\n",
    "            assert len(self.env.action_space.shape) == 1\n",
    "            self.act_dim = self.env.action_space.shape[0]\n",
    "        elif isinstance(self.env.action_space, gym.spaces.discrete.Discrete):\n",
    "            self.act_dim = self.env.action_space.n\n",
    "        else:\n",
    "            raise ValueError(\"Wrong action space!\")\n",
    "\n",
    "        self.eps = self.config['eps']\n",
    "\n",
    "        # You need to setup the parameter for your function approximator.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = None\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the \"\n",
    "            \"Trainer._initialize_parameters() function.\")\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Preprocess the state (observation).\n",
    "\n",
    "        If the environment provides discrete observation (state), transform\n",
    "        it to one-hot form. For example, the environment FrozenLake-v0\n",
    "        provides an integer in [0, ..., 15] denotes the 16 possible states.\n",
    "        We transform it to one-hot style:\n",
    "\n",
    "        original state 0 -> one-hot vector [1, 0, 0, 0, 0, 0, 0, 0, ...]\n",
    "        original state 1 -> one-hot vector [0, 1, 0, 0, 0, 0, 0, 0, ...]\n",
    "        original state 15 -> one-hot vector [0, ..., 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "        If the observation space is continuous, then you should do nothing.\n",
    "        \"\"\"\n",
    "        if not self.discrete_obs:\n",
    "            return state\n",
    "        else:\n",
    "            new_state = np.zeros((self.obs_dim,))\n",
    "            new_state[state] = 1\n",
    "        return new_state\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"Approximate the state value of given state.\n",
    "        This is a private function.\n",
    "        Note that you should NOT preprocess the state here.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.compute_values() function.\")\n",
    "\n",
    "    def compute_action(self, processed_state, eps=None):\n",
    "        \"\"\"Compute the action given the state. Note that the input\n",
    "        is the processed state.\"\"\"\n",
    "\n",
    "        values = self.compute_values(processed_state)\n",
    "        assert values.ndim == 1, values.shape\n",
    "\n",
    "        if eps is None:\n",
    "            eps = self.eps\n",
    "\n",
    "        # [TODO] Implement the epsilon-greedy policy here. We have `eps`\n",
    "        #  probability to choose a uniformly random action in action_space,\n",
    "        #  otherwise choose action that maximizes the values.\n",
    "        # Hint: Use the function of self.env.action_space to sample random\n",
    "        # action.\n",
    "        if np.random.uniform(0,1)<self.eps:\n",
    "            return np.random.choice(self.env.action_space.n)\n",
    "        else:\n",
    "            return np.argmax(values)\n",
    "\n",
    "    def evaluate(self, num_episodes=50, *args, **kwargs):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 50 episodes.\"\"\"\n",
    "        policy = lambda raw_state: self.compute_action(\n",
    "            self.process_state(raw_state), eps=0.0)\n",
    "        if \"MetaDrive\" in self.env_name:\n",
    "            kwargs[\"existing_env\"] = self.env\n",
    "        result = evaluate(policy, num_episodes, seed=self.seed,\n",
    "                          env_name=self.env_name, *args, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def compute_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.compute_gradient() function.\")\n",
    "\n",
    "    def apply_gradient(self, *args, **kwargs):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"You need to override the Trainer.apply_gradient() function.\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for a random policy in 500 episodes in CartPole-v0:  22.554\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestTrainer(AbstractTrainer):\n",
    "    \"\"\"This class is used for testing. We don't really train anything.\"\"\"\n",
    "    def compute_values(self, state):\n",
    "        return np.random.random_sample(size=self.act_dim)\n",
    "    def initialize_parameters(self):\n",
    "        self.parameters = np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "    \n",
    "t = TestTrainer(dict(env_name=\"CartPole-v0\"))\n",
    "obs = t.env.observation_space.sample()\n",
    "processed = t.process_state(obs)\n",
    "assert processed.shape == (4, )\n",
    "assert np.all(processed == obs)\n",
    "# Test compute_action\n",
    "values = t.compute_values(processed)\n",
    "correct_act = np.argmax(values)\n",
    "assert t.compute_action(processed, eps=0) == correct_act\n",
    "print(\"Average episode reward for a random policy in 500 episodes in CartPole-v0: \",\n",
    "      t.evaluate(num_episodes=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "linear_approximator_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class LinearTrainer(AbstractTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_approximator_config)\n",
    "\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.max_episode_length = self.config[\"max_episode_length\"]\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.gamma = self.config[\"gamma\"]\n",
    "        self.n = self.config[\"n\"]\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.parameters, which is two dimensional matrix,\n",
    "        #  and subjects to a normal distribution with scale\n",
    "        #  config[\"parameter_std\"].\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.parameters = std*np.random.random_sample(size=(self.obs_dim, self.act_dim))\n",
    "        \n",
    "        print(\"Initialize parameters with shape: {}.\".format(\n",
    "            self.parameters.shape))\n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        # [TODO] Compute the value for each potential action. Note that you\n",
    "        #  should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        \n",
    "        ret = np.dot(self.parameters.transpose(),processed_state.transpose())\n",
    "        \n",
    "        return ret\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Please implement the n-step Sarsa algorithm presented in Chapter 10.2\n",
    "        of the textbook. You algorithm should reduce the convention one-step\n",
    "        Sarsa when n = 1. That is:\n",
    "            TD = r_t + gamma * Q(s_t+1, a_t+1) - Q(s_t, a_t)\n",
    "            Q(s_t, a_t) = Q(s_t, a_t) + learning_rate * TD\n",
    "        \"\"\"\n",
    "        s = self.env.reset()\n",
    "        processed_s = self.process_state(s)\n",
    "        processed_states = [processed_s]\n",
    "        rewards = [0.0]\n",
    "        actions = [self.compute_action(processed_s)]\n",
    "        T = float(\"inf\")\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            if t < T:\n",
    "                # [TODO]  When the termination is not reach, apply action,\n",
    "                #  process state, record state / reward / action to the\n",
    "                #  lists defined above, and deal with termination.\n",
    "                next_state, reward, done, info  = self.env.step(actions[t])\n",
    "                                \n",
    "                processed_s = self.process_state(next_state)\n",
    "                processed_states.append(processed_s)\n",
    "                rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    next_act = self.compute_action(processed_s)\n",
    "                    actions.append(next_act)\n",
    "\n",
    "            tau = t - self.n + 1\n",
    "            if tau >= 0:\n",
    "                gradient = self.compute_gradient(\n",
    "                    processed_states, actions, rewards, tau, T\n",
    "                )\n",
    "                self.apply_gradient(gradient)\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        \"\"\"Compute the gradient\"\"\"\n",
    "        n = self.n\n",
    "\n",
    "        # [TODO] Compute the approximation goal, the truth state action value\n",
    "        #  G. It is a n-step discounted sum of rewards. Refer to Chapter 10.2\n",
    "        #  of the textbook.\n",
    "        # [HINT] G have two parts: the accumuted reward computed from step tau to \n",
    "        #  step tau+n, and the possible state value at time step tau+n, if the episode\n",
    "        #  is not terminated. Remember to apply the discounter factor (\\gamma^n) to\n",
    "        #  the second part of G if applicable.\n",
    "        G = 0\n",
    "        for t in reversed(range(n)):\n",
    "            G = (G**self.gamma)+rewards[t+tau]\n",
    "                \n",
    "        if tau + n < T:\n",
    "            # [TODO] If at time step tau + n the episode is not terminated,\n",
    "            # then we should add the state action value at tau + n\n",
    "            # to the G.\n",
    "            G += (self.gamma**n)*max(self.compute_values(processed_states[tau+n]))\n",
    "        \n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the weights can be separated into two\n",
    "        # parts (the chain rule):\n",
    "        #     dLoss / dweight = (dLoss / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predicted value (Q(s_t, a_t)) to be the loss.\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))\n",
    "        # [TODO] fill the propoer value of loss_grad, denoting the gradient\n",
    "        # of the MSE w.r.t. the output of the linear function.\n",
    "        loss_grad[:,0] = -(G*np.ones(2)-self.compute_values(processed_states[tau]))\n",
    "\n",
    "        # [TODO] compute the value of value_grad, denoting the gradient of\n",
    "        # the output of the linear function w.r.t. the parameters.\n",
    "        value_grad = np.zeros((self.obs_dim, 1))\n",
    "        value_grad[:,0] = processed_states[tau]\n",
    "\n",
    "        assert loss_grad.shape == (self.act_dim, 1)\n",
    "        assert value_grad.shape == (self.obs_dim, 1)\n",
    "\n",
    "        # [TODO] merge two gradients to get the gradient of loss w.r.t. the\n",
    "        # parameters.\n",
    "        gradient = np.dot(value_grad,loss_grad.transpose())\n",
    "        pass\n",
    "    \n",
    "        return gradient\n",
    "\n",
    "    def apply_gradient(self, gradient):\n",
    "        \"\"\"Apply the gradient to the parameter.\"\"\"\n",
    "        assert gradient.shape == self.parameters.shape, (\n",
    "            gradient.shape, self.parameters.shape)\n",
    "        # [TODO] apply the gradient to self.parameters\n",
    "        self.parameters -= self.learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "Now your codes should be bug-free.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Build the test trainer.\n",
    "test_trainer = LinearTrainer(dict(parameter_std=0.0))\n",
    "\n",
    "# Test self.parameters.\n",
    "assert test_trainer.parameters.std() == 0.0, \\\n",
    "    \"Parameters should subjects to a normal distribution with standard \" \\\n",
    "    \"deviation config['parameter_std'], but you have {}.\" \\\n",
    "    \"\".format(test_trainer.parameters.std())\n",
    "assert test_trainer.parameters.mean() == 0, \\\n",
    "    \"Parameters should subjects to a normal distribution with mean 0. \" \\\n",
    "    \"But you have {}.\".format(test_trainer.parameters.mean())\n",
    "\n",
    "# Test compute_values\n",
    "fake_state = test_trainer.env.observation_space.sample()\n",
    "processed_state = test_trainer.process_state(fake_state)\n",
    "assert processed_state.shape == (test_trainer.obs_dim, ), processed_state.shape\n",
    "values = test_trainer.compute_values(fake_state)\n",
    "assert values.shape == (test_trainer.act_dim, ), values.shape\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "# Test compute_gradient\n",
    "tmp_gradient = test_trainer.compute_gradient(\n",
    "    [processed_state]*10, [test_trainer.env.action_space.sample()]*10, [0.0]*10, 2, 5)\n",
    "assert tmp_gradient.shape == test_trainer.parameters.shape\n",
    "\n",
    "test_trainer.train()\n",
    "print(\"Now your codes should be bug-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "(0.3s,+0.3s)\tIteration 0, current mean episode reward is 145.16. \n",
      "(3.4s,+3.1s)\tIteration 1000, current mean episode reward is 21.94. \n",
      "(4.8s,+1.4s)\tIteration 2000, current mean episode reward is 20.86. \n",
      "(6.2s,+1.4s)\tIteration 3000, current mean episode reward is 18.38. \n",
      "(7.3s,+1.1s)\tIteration 4000, current mean episode reward is 11.16. \n",
      "(8.1s,+0.8s)\tIteration 5000, current mean episode reward is 11.38. \n",
      "(8.9s,+0.8s)\tIteration 6000, current mean episode reward is 11.4. \n",
      "(9.7s,+0.8s)\tIteration 7000, current mean episode reward is 11.06. \n",
      "(10.5s,+0.8s)\tIteration 8000, current mean episode reward is 12.04. \n",
      "(11.3s,+0.8s)\tIteration 9000, current mean episode reward is 11.3. \n",
      "(12.1s,+0.8s)\tIteration 10000, current mean episode reward is 11.24. \n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_trainer, _ = run(LinearTrainer, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "))\n",
    "\n",
    "# It's OK to see bad performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward for your linear agent in CartPole-v0:  12.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your linear agent in CartPole-v0: \",\n",
    "      linear_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "linear_fc_config = merge_config(dict(\n",
    "    polynomial_order=1,\n",
    "), linear_approximator_config)\n",
    "\n",
    "\n",
    "def polynomial_feature(sequence, order=1):\n",
    "    \"\"\"\n",
    "    Construct the order-n polynomial-basis feature of the state.\n",
    "    Refer to Chapter 9.5.1 of the textbook. \n",
    "    We expect to get a vector of length `(order+1)^k` as the output,\n",
    "    wherein `k` is the dimensions of the state.\n",
    "\n",
    "    For example:\n",
    "    When the state is [2, 3, 4] (so k=3), \n",
    "    the first order polynomial feature of the state is \n",
    "    [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        2 * 3 = 6,\n",
    "        2 * 4 = 8,\n",
    "        3 * 4 = 12,\n",
    "        2 * 3 * 4 = 24\n",
    "    ].\n",
    "    \n",
    "    We have `(1+1)^3=8` output dimensions.\n",
    "\n",
    "    Note: it is not necessary to follow the ascending order.\n",
    "    \"\"\"\n",
    "    # [TODO] finish this function.\n",
    "    output = []\n",
    "    if len(sequence)==3:\n",
    "        for i in range(order+1):\n",
    "            temp1 = sequence[0]**i\n",
    "            for j in range(order+1):\n",
    "                temp2 = temp1*sequence[1]**j\n",
    "                for k in range(order+1):\n",
    "                    output.append(temp2*sequence[2]**k)\n",
    "    if len(sequence)==4:\n",
    "        for i in range(order+1):\n",
    "            temp1 = sequence[0]**i\n",
    "            for j in range(order+1):\n",
    "                temp2 = temp1*sequence[1]**j\n",
    "                for k in range(order+1):\n",
    "                    temp3 = temp2*sequence[2]**k\n",
    "                    for r in range(order+1):\n",
    "                        output.append(temp3*sequence[3]**r)        \n",
    "    return output\n",
    "\n",
    "assert sorted(polynomial_feature([2, 3, 4])) == [1, 2, 3, 4, 6, 8, 12, 24]\n",
    "assert len(polynomial_feature([2, 3, 4], 2)) == 27\n",
    "assert len(polynomial_feature([2, 3, 4], 3)) == 64\n",
    "\n",
    "class LinearTrainerWithFeatureConstruction(LinearTrainer):\n",
    "    \"\"\"In this class, we will expand the dimension of the state.\n",
    "    This procedure is done at self.process_state function.\n",
    "    The modification of self.obs_dim and the shape of parameters\n",
    "    is also needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, linear_fc_config)\n",
    "        # Initialize the abstract class.\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.polynomial_order = self.config[\"polynomial_order\"]\n",
    "\n",
    "        # Expand the size of observation\n",
    "        self.obs_dim = (self.polynomial_order + 1) ** self.obs_dim\n",
    "\n",
    "        # Since we change self.obs_dim, reset the parameters.\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def process_state(self, state):\n",
    "        \"\"\"Please finish the polynomial function.\"\"\"\n",
    "        processed = polynomial_feature(state, self.polynomial_order)\n",
    "        processed = np.asarray(processed)\n",
    "        assert len(processed) == self.obs_dim, processed.shape\n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize parameters with shape: (4, 2).\n",
      "Initialize parameters with shape: (16, 2).\n",
      "(0.1s,+0.1s)\tIteration 0, current mean episode reward is 10.94. \n",
      "(4.4s,+4.2s)\tIteration 1000, current mean episode reward is 26.74. \n",
      "(11.2s,+6.8s)\tIteration 2000, current mean episode reward is 34.38. \n",
      "(16.3s,+5.1s)\tIteration 3000, current mean episode reward is 20.1. \n",
      "(20.8s,+4.5s)\tIteration 4000, current mean episode reward is 19.34. \n",
      "(25.3s,+4.5s)\tIteration 5000, current mean episode reward is 19.8. \n",
      "(29.5s,+4.2s)\tIteration 6000, current mean episode reward is 19.98. \n",
      "(33.7s,+4.1s)\tIteration 7000, current mean episode reward is 21.54. \n",
      "(38.0s,+4.3s)\tIteration 8000, current mean episode reward is 20.0. \n",
      "(42.0s,+4.1s)\tIteration 9000, current mean episode reward is 18.0. \n",
      "(46.1s,+4.1s)\tIteration 10000, current mean episode reward is 22.8. \n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "linear_fc_trainer, _ = run(LinearTrainerWithFeatureConstruction, dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=1000, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    polynomial_order=1,\n",
    "    n=3,\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert linear_fc_trainer.evaluate() > 20.0, \"The best episode reward happening \" \\\n",
    "    \"during training should be greater than the random baseline. That is more than 20+.\"\n",
    "\n",
    "# This cell should be finished within 10 minitines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In CartPole-v0, the average episode reward for the value estimator with feature construction is:  11.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\n",
    "    \"In CartPole-v0, the average episode reward for the value estimator \"\n",
    "    \"with feature construction is: \",\n",
    "      linear_fc_trainer.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Build the algorithm-specify config.\n",
    "mlp_trainer_config = merge_config(dict(\n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.01,\n",
    "    hidden_dim=100,\n",
    "    n=3,\n",
    "    clip_norm=1.0,\n",
    "    clip_gradient=True\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class MLPTrainer(LinearTrainer):\n",
    "    def __init__(self, config):\n",
    "        config = merge_config(config, mlp_trainer_config)\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        super().__init__(config)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # [TODO] Initialize self.hidden_parameters and self.output_parameters,\n",
    "        #  which are two dimensional matrices, and subject to normal\n",
    "        #  distributions with scale config[\"parameter_std\"]\n",
    "        std = self.config[\"parameter_std\"]\n",
    "        self.hidden_parameters = std*np.random.random_sample(size=(self.obs_dim, self.hidden_dim))\n",
    "        self.output_parameters = std*np.random.random_sample(size=(self.hidden_dim, self.act_dim))\n",
    "        \n",
    "\n",
    "    def compute_values(self, processed_state):\n",
    "        \"\"\"[TODO] Compute the value for each potential action. Note that you\n",
    "        should NOT preprocess the state here.\"\"\"\n",
    "        assert processed_state.ndim == 1, processed_state.shape\n",
    "        activation = self.compute_activation(processed_state)\n",
    "        values = np.dot(self.output_parameters.transpose(),activation)  \n",
    "        \n",
    "        return values\n",
    "\n",
    "    def compute_activation(self, processed_state):\n",
    "        \"\"\"[TODO] Compute the action values values.\n",
    "        Given a processed state, first we need to compute the activtaion\n",
    "        (the output of hidden layer). Then we compute the values (the output of\n",
    "        the output layer).\n",
    "        \"\"\"\n",
    "        activation = np.dot(self.hidden_parameters.transpose(),processed_state)\n",
    "\n",
    "    \n",
    "        return activation\n",
    "\n",
    "    def compute_gradient(self, processed_states, actions, rewards, tau, T):\n",
    "        n = self.n\n",
    "        \n",
    "        # [TODO] compute the target value.\n",
    "        # Hint: copy your codes in LinearTrainer.\n",
    "        G = 0\n",
    "        #for t in reversed(range(n)):\n",
    "            #G = (G**self.gamma)+rewards[t+tau]\n",
    "        for t in range(tau, tau+n-1):\n",
    "            G += (self.gamma**(t-tau))*rewards[t]    \n",
    "            \n",
    "                  \n",
    "        if tau + n < T:\n",
    "            G += (self.gamma**n)*max(self.compute_values(processed_states[tau+n]))\n",
    "\n",
    "        # Denote the state-action value function Q, then the loss of\n",
    "        # prediction error w.r.t. the output layer weights can be \n",
    "        # separated into two parts (the chain rule):\n",
    "        #     dError / dweight = (dError / dQ) * (dQ / dweight)\n",
    "        # We call the first one loss_grad, and the latter one\n",
    "        # value_grad. We consider the Mean Square Error between the target\n",
    "        # value (G) and the predict value (Q(s_t, a_t)) to be the loss.\n",
    "        #cur_state = processed_states[tau]\n",
    "\n",
    "        loss_grad = np.zeros((self.act_dim, 1))  # [act_dim, 1]\n",
    "        # [TODO] compute loss_grad\n",
    "        loss_grad[:,0] = -(G*np.ones(2)-self.compute_values(processed_states[tau]))\n",
    "        \n",
    "        # [TODO] compute the gradient of output layer parameters\n",
    "        value_gradient = np.zeros((self.hidden_dim, 1))\n",
    "        value_gradient[:,0] = self.compute_activation(processed_states[tau])\n",
    "        output_gradient = np.dot(value_gradient, loss_grad.transpose())\n",
    "\n",
    "        \n",
    "        # [TODO] compute the gradient of hidden layer parameters\n",
    "        # Hint: using chain rule and derive the formulation\n",
    "        cur_state = np.zeros((self.obs_dim, 1))\n",
    "        cur_state[:,0] = processed_states[tau]\n",
    "        activate_gradient = np.dot(self.output_parameters,loss_grad)      \n",
    "        hidden_gradient = np.dot(cur_state,activate_gradient.transpose()) \n",
    "    \n",
    "        assert np.all(np.isfinite(output_gradient)), \\\n",
    "            \"Invalid value occurs in output_gradient! {}\".format(\n",
    "                output_gradient)\n",
    "        assert np.all(np.isfinite(hidden_gradient)), \\\n",
    "            \"Invalid value occurs in hidden_gradient! {}\".format(\n",
    "                hidden_gradient)\n",
    "        return [hidden_gradient, output_gradient]\n",
    "\n",
    "    def apply_gradient(self, gradients):\n",
    "        \"\"\"Apply the gradientss to the two layers' parameters.\"\"\"\n",
    "        assert len(gradients) == 2\n",
    "        hidden_gradient, output_gradient = gradients\n",
    "\n",
    "        assert output_gradient.shape == (self.hidden_dim, self.act_dim)\n",
    "        assert hidden_gradient.shape == (self.obs_dim, self.hidden_dim)\n",
    "        \n",
    "        # [TODO] Implement the clip gradient mechansim\n",
    "        # Hint: when the old gradient has norm less that clip_norm,\n",
    "        #  then nothing happens. Otherwise shrink the gradient to\n",
    "        #  make its norm equal to clip_norm.\n",
    "        if self.config[\"clip_gradient\"]:\n",
    "            clip_norm = self.config[\"clip_norm\"]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            if np.linalg.norm(output_gradient,2)>clip_norm:\n",
    "                output_gradient = output_gradient*clip_norm/np.linalg.norm(output_gradient,2)\n",
    "            if np.linalg.norm(hidden_gradient,2)>clip_norm:\n",
    "                hidden_gradient = hidden_gradient*clip_norm/np.linalg.norm(hidden_gradient,2)\n",
    "\n",
    "        # [TODO] update the parameters\n",
    "        # Hint: Remember to check the sign when applying the gradient\n",
    "        #  into the parameters. Should you add or minus the gradients?\n",
    "        self.learning_rate = self.config[\"learning_rate\"]\n",
    "        self.hidden_parameters -= self.learning_rate*hidden_gradient\n",
    "        self.output_parameters -= self.learning_rate*output_gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's see what happen if gradient clipping is not enable!\n",
      "\n",
      "(0.0s,+0.0s)\tIteration 0, current mean episode reward is 11.34. \n",
      "(0.4s,+0.3s)\tIteration 100, current mean episode reward is 11.74. \n",
      "(0.5s,+0.1s)\tIteration 200, current mean episode reward is 11.2. \n",
      "(0.7s,+0.1s)\tIteration 300, current mean episode reward is 11.78. \n",
      "(0.8s,+0.1s)\tIteration 400, current mean episode reward is 11.12. \n",
      "(0.9s,+0.1s)\tIteration 500, current mean episode reward is 10.6. \n",
      "(1.1s,+0.2s)\tIteration 600, current mean episode reward is 13.34. \n",
      "(1.2s,+0.1s)\tIteration 700, current mean episode reward is 11.92. \n",
      "(1.4s,+0.2s)\tIteration 800, current mean episode reward is 10.94. \n",
      "(1.8s,+0.4s)\tIteration 900, current mean episode reward is 50.04. \n",
      "(2.0s,+0.2s)\tIteration 1000, current mean episode reward is 11.02. \n",
      "(2.1s,+0.1s)\tIteration 1100, current mean episode reward is 11.32. \n",
      "(2.2s,+0.1s)\tIteration 1200, current mean episode reward is 11.04. \n",
      "(2.4s,+0.2s)\tIteration 1300, current mean episode reward is 11.38. \n",
      "(2.5s,+0.1s)\tIteration 1400, current mean episode reward is 10.66. \n",
      "(2.7s,+0.1s)\tIteration 1500, current mean episode reward is 11.1. \n",
      "(2.8s,+0.1s)\tIteration 1600, current mean episode reward is 10.72. \n",
      "(3.0s,+0.1s)\tIteration 1700, current mean episode reward is 11.1. \n",
      "(3.1s,+0.1s)\tIteration 1800, current mean episode reward is 11.66. \n",
      "(3.2s,+0.1s)\tIteration 1900, current mean episode reward is 11.46. \n",
      "(3.4s,+0.1s)\tIteration 2000, current mean episode reward is 11.26. \n",
      "(3.5s,+0.1s)\tIteration 2100, current mean episode reward is 11.42. \n",
      "(3.6s,+0.1s)\tIteration 2200, current mean episode reward is 11.28. \n",
      "(3.8s,+0.1s)\tIteration 2300, current mean episode reward is 11.66. \n",
      "(3.9s,+0.1s)\tIteration 2400, current mean episode reward is 10.92. \n",
      "(4.1s,+0.1s)\tIteration 2500, current mean episode reward is 10.7. \n",
      "(4.2s,+0.1s)\tIteration 2600, current mean episode reward is 11.0. \n",
      "(4.3s,+0.1s)\tIteration 2700, current mean episode reward is 11.44. \n",
      "(4.5s,+0.1s)\tIteration 2800, current mean episode reward is 11.16. \n",
      "(4.6s,+0.1s)\tIteration 2900, current mean episode reward is 10.78. \n",
      "(4.8s,+0.1s)\tIteration 3000, current mean episode reward is 11.3. \n",
      "\n",
      "We expect to see bad performance (<195). The performance without gradient clipping: 11.46.\n",
      "Try next cell to see the impact of gradient clipping.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if gradient clipping is not enable!\\n\")\n",
    "try:\n",
    "    failed_mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "        max_iteration=3000,\n",
    "        evaluate_interval=100, \n",
    "        parameter_std=0.01,\n",
    "        learning_rate=0.001,\n",
    "        hidden_dim=100,\n",
    "        clip_gradient=False,  # <<< Gradient clipping is OFF!\n",
    "        env_name=\"CartPole-v0\"\n",
    "    ), reward_threshold=195.0)\n",
    "    print(\"\\nWe expect to see bad performance (<195). \"\n",
    "          \"The performance without gradient clipping: {}.\"\n",
    "          \"\".format(failed_mlp_trainer.evaluate()))\n",
    "except AssertionError as e:\n",
    "    print(traceback.format_exc())\n",
    "    print(\"Infinity happen during training. It's OK since the gradient is not bounded.\")\n",
    "finally:\n",
    "    print(\"Try next cell to see the impact of gradient clipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now let's see what happen if gradient clipping is enable!\n",
      "\n",
      "(0.0s,+0.0s)\tIteration 0, current mean episode reward is 11.34. \n",
      "(0.7s,+0.6s)\tIteration 100, current mean episode reward is 11.74. \n",
      "(0.9s,+0.2s)\tIteration 200, current mean episode reward is 11.2. \n",
      "(1.2s,+0.3s)\tIteration 300, current mean episode reward is 11.78. \n",
      "(1.5s,+0.3s)\tIteration 400, current mean episode reward is 11.12. \n",
      "(1.7s,+0.3s)\tIteration 500, current mean episode reward is 10.6. \n",
      "(2.0s,+0.3s)\tIteration 600, current mean episode reward is 13.34. \n",
      "(2.2s,+0.3s)\tIteration 700, current mean episode reward is 11.92. \n",
      "(2.5s,+0.3s)\tIteration 800, current mean episode reward is 10.94. \n",
      "(3.1s,+0.5s)\tIteration 900, current mean episode reward is 50.04. \n",
      "(3.4s,+0.3s)\tIteration 1000, current mean episode reward is 11.02. \n",
      "(3.7s,+0.3s)\tIteration 1100, current mean episode reward is 11.32. \n",
      "(3.9s,+0.3s)\tIteration 1200, current mean episode reward is 11.04. \n",
      "(4.2s,+0.3s)\tIteration 1300, current mean episode reward is 11.38. \n",
      "(4.5s,+0.2s)\tIteration 1400, current mean episode reward is 10.66. \n",
      "(4.7s,+0.2s)\tIteration 1500, current mean episode reward is 11.1. \n",
      "(5.0s,+0.2s)\tIteration 1600, current mean episode reward is 10.72. \n",
      "(5.2s,+0.2s)\tIteration 1700, current mean episode reward is 11.1. \n",
      "(5.5s,+0.3s)\tIteration 1800, current mean episode reward is 11.66. \n",
      "(5.7s,+0.2s)\tIteration 1900, current mean episode reward is 11.46. \n",
      "(6.0s,+0.3s)\tIteration 2000, current mean episode reward is 11.26. \n",
      "(6.2s,+0.2s)\tIteration 2100, current mean episode reward is 11.42. \n",
      "(6.5s,+0.2s)\tIteration 2200, current mean episode reward is 11.28. \n",
      "(6.7s,+0.3s)\tIteration 2300, current mean episode reward is 11.66. \n",
      "(7.0s,+0.3s)\tIteration 2400, current mean episode reward is 10.92. \n",
      "(7.3s,+0.3s)\tIteration 2500, current mean episode reward is 10.7. \n",
      "(7.6s,+0.3s)\tIteration 2600, current mean episode reward is 11.0. \n",
      "(7.9s,+0.3s)\tIteration 2700, current mean episode reward is 11.44. \n",
      "(8.2s,+0.3s)\tIteration 2800, current mean episode reward is 11.16. \n",
      "(8.5s,+0.3s)\tIteration 2900, current mean episode reward is 10.78. \n",
      "(8.8s,+0.3s)\tIteration 3000, current mean episode reward is 11.3. \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Check your codes. Your agent should achieve {} reward in 200 iterations.But it achieve {} reward in evaluation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-786028ff0bf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m ), reward_threshold=195.0)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mmlp_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m195.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Check your codes. \"\u001b[0m     \u001b[1;34m\"Your agent should achieve {} reward in 200 iterations.\"\u001b[0m     \u001b[1;34m\"But it achieve {} reward in evaluation.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# In our implementation, the task is solved in 200 iterations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Check your codes. Your agent should achieve {} reward in 200 iterations.But it achieve {} reward in evaluation."
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Now let's see what happen if gradient clipping is enable!\\n\")\n",
    "mlp_trainer, _ = run(MLPTrainer, dict(\n",
    "    max_iteration=3000,\n",
    "    evaluate_interval=100, \n",
    "    parameter_std=0.01,\n",
    "    learning_rate=0.001,\n",
    "    hidden_dim=100,\n",
    "    clip_gradient=True,  # <<< Gradient clipping is ON!\n",
    "    env_name=\"CartPole-v0\"\n",
    "), reward_threshold=195.0)\n",
    "\n",
    "assert mlp_trainer.evaluate() > 195.0, \"Check your codes. \" \\\n",
    "    \"Your agent should achieve {} reward in 200 iterations.\" \\\n",
    "    \"But it achieve {} reward in evaluation.\"\n",
    "\n",
    "# In our implementation, the task is solved in 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
